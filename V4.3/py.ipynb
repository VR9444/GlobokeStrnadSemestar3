{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d633382c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports & basic setup\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"mps\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a39770ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Replay buffer and Q-network\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states      = np.array(states, dtype=np.float32)\n",
    "        next_states = np.array(next_states, dtype=np.float32)\n",
    "        actions     = np.array(actions, dtype=np.int64)\n",
    "        rewards     = np.array(rewards, dtype=np.float32)\n",
    "        dones       = np.array(dones, dtype=np.float32)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9916129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Agent that can do DQN or DDQN depending on use_ddqn flag\n",
    "\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        use_ddqn=False,\n",
    "        gamma=0.99,\n",
    "        lr=3e-4,\n",
    "        batch_size=64,\n",
    "        buffer_capacity=100_000,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.05,\n",
    "        epsilon_decay=0.995,\n",
    "        target_update_every=30,\n",
    "        device=None,\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.use_ddqn = use_ddqn\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay= epsilon_decay\n",
    "        self.target_update_every = target_update_every\n",
    "\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Online + target networks\n",
    "        self.q_net = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_net = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.train_steps = 0\n",
    "\n",
    "    def select_action(self, state, explore=True):\n",
    "        # epsilon-greedy\n",
    "        if explore and random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "\n",
    "        state_t = torch.as_tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state_t)\n",
    "            action = int(torch.argmax(q_values, dim=1).item())\n",
    "        return action\n",
    "\n",
    "    def update(self):\n",
    "        # replay warmup â€“ wait until we have enough diverse data\n",
    "        if len(self.replay_buffer) < max(self.batch_size, 1000):\n",
    "            return None\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        states_t      = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        next_states_t = torch.as_tensor(next_states, dtype=torch.float32, device=self.device)\n",
    "        actions_t     = torch.as_tensor(actions, dtype=torch.int64, device=self.device).unsqueeze(1)\n",
    "        rewards_t     = torch.as_tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones_t       = torch.as_tensor(dones, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Q(s,a) for taken actions\n",
    "        q_values = self.q_net(states_t).gather(1, actions_t).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if self.use_ddqn:\n",
    "                # --- DDQN target ---\n",
    "                # 1) Online net selects best action at next state\n",
    "                next_q_online = self.q_net(next_states_t)  # Q_theta(s', a)\n",
    "                next_actions  = torch.argmax(next_q_online, dim=1, keepdim=True)\n",
    "\n",
    "                # 2) Target net evaluates that action\n",
    "                next_q_target = self.target_net(next_states_t).gather(1, next_actions).squeeze(1)\n",
    "            else:\n",
    "                # --- Plain DQN target ---\n",
    "                # target net both selects and evaluates best next action\n",
    "                next_q_target = self.target_net(next_states_t).max(dim=1)[0]\n",
    "\n",
    "            target_q = rewards_t + self.gamma * next_q_target * (1.0 - dones_t)\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, target_q)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.train_steps += 1\n",
    "        if self.train_steps % self.target_update_every == 0:\n",
    "            self.update_target_net()\n",
    "\n",
    "        return float(loss.item())\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa6e0f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training loop for CartPole or Acrobot with DQN / DDQN\n",
    "\n",
    "def get_env_defaults(env_name: str):\n",
    "    \"\"\"\n",
    "    Returns (max_steps_per_episode, solved_score) defaults for supported envs.\n",
    "    \"\"\"\n",
    "    if \"CartPole\" in env_name:\n",
    "        # reward +1 per timestep, max ~500\n",
    "        return 500, 450.0   # solved if avg >= 450\n",
    "    elif \"Acrobot\" in env_name:\n",
    "        # reward -1 per timestep, best ~ -100 or better\n",
    "        return 500, -100.0  # solved if avg >= -100 (less negative)\n",
    "    else:\n",
    "        # generic fallback\n",
    "        return 1000, 200.0\n",
    "\n",
    "\n",
    "def train_agent(\n",
    "    env_name=\"CartPole-v1\",\n",
    "    use_ddqn=False,\n",
    "    num_episodes=500,\n",
    "    max_steps_per_episode=None,\n",
    "    solved_score=None,\n",
    "    print_every=10,\n",
    "):\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # defaults per environment if not provided\n",
    "    default_max_steps, default_solved = get_env_defaults(env_name)\n",
    "    if max_steps_per_episode is None:\n",
    "        max_steps_per_episode = default_max_steps\n",
    "    if solved_score is None:\n",
    "        solved_score = default_solved\n",
    "\n",
    "    # slightly different epsilon end for harder env (Acrobot)\n",
    "    if \"Acrobot\" in env_name:\n",
    "        eps_end = 0.05\n",
    "    else:\n",
    "        eps_end = 0.05\n",
    "\n",
    "    agent = Agent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        use_ddqn=use_ddqn,\n",
    "        gamma=0.99,\n",
    "        lr=5e-4,\n",
    "        batch_size=64,\n",
    "        buffer_capacity=100_000,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=eps_end,\n",
    "        epsilon_decay=0.995,\n",
    "        target_update_every=20,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    algo_name = \"DDQN\" if use_ddqn else \"DQN\"\n",
    "    print(f\"Training {algo_name} on {env_name} for up to {num_episodes} episodes\")\n",
    "\n",
    "    scores = []\n",
    "    moving_avgs = []\n",
    "    eps_history = []\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0.0\n",
    "\n",
    "        for t in range(max_steps_per_episode):\n",
    "            action = agent.select_action(state, explore=True)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            loss = agent.update()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores.append(total_reward)\n",
    "        moving_avg = np.mean(scores[-10:])\n",
    "        moving_avgs.append(moving_avg)\n",
    "\n",
    "        agent.decay_epsilon()\n",
    "        eps_history.append(agent.epsilon)\n",
    "\n",
    "        if episode % print_every == 0:\n",
    "            print(\n",
    "                f\"[{algo_name} | {env_name}] \"\n",
    "                f\"Ep {episode:4d} | \"\n",
    "                f\"Score: {total_reward:7.2f} | \"\n",
    "                f\"Avg(10): {moving_avg:7.2f} | \"\n",
    "                f\"Eps: {agent.epsilon:.3f}\"\n",
    "            )\n",
    "\n",
    "        # stop condition based on moving average\n",
    "        if episode >= 10 and moving_avg >= solved_score:\n",
    "            print(\n",
    "                f\"[{algo_name} | {env_name}] \"\n",
    "                f\"Solved in {episode} episodes, moving average = {moving_avg:.2f}\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Plot scores\n",
    "    plt.figure()\n",
    "    plt.plot(scores, label=\"Score per episode\")\n",
    "    plt.plot(moving_avgs, label=\"Moving average (last 10)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Return\")\n",
    "    plt.title(f\"{algo_name} on {env_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{algo_name}_{env_name}_scores.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot epsilon decay\n",
    "    plt.figure()\n",
    "    plt.plot(eps_history)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Epsilon\")\n",
    "    plt.title(f\"Epsilon decay ({algo_name} on {env_name})\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{algo_name}_{env_name}_epsilon.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    return agent, scores, moving_avgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75e82f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: GIF generation for a trained agent\n",
    "\n",
    "def make_gif(\n",
    "    agent: Agent,\n",
    "    env_name: str,\n",
    "    filename: str,\n",
    "    max_steps_per_episode: int = None,\n",
    "    fps: int = 30,\n",
    "):\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    default_max_steps, _ = get_env_defaults(env_name)\n",
    "    if max_steps_per_episode is None:\n",
    "        max_steps_per_episode = default_max_steps\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    for t in range(max_steps_per_episode):\n",
    "        # Greedy policy (no exploration)\n",
    "        action = agent.select_action(state, explore=False)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    imageio.mimsave(filename, frames, fps=fps)\n",
    "    print(f\"Saved GIF to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ea5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DDQN on Acrobot-v1 for up to 800 episodes\n",
      "[DDQN | Acrobot-v1] Ep   10 | Score: -500.00 | Avg(10): -490.60 | Eps: 0.951\n",
      "[DDQN | Acrobot-v1] Ep   20 | Score: -500.00 | Avg(10): -475.50 | Eps: 0.905\n",
      "[DDQN | Acrobot-v1] Ep   30 | Score: -500.00 | Avg(10): -496.20 | Eps: 0.860\n",
      "[DDQN | Acrobot-v1] Ep   40 | Score: -500.00 | Avg(10): -482.40 | Eps: 0.818\n",
      "[DDQN | Acrobot-v1] Ep   50 | Score: -500.00 | Avg(10): -500.00 | Eps: 0.778\n",
      "[DDQN | Acrobot-v1] Ep   60 | Score: -238.00 | Avg(10): -457.80 | Eps: 0.740\n",
      "[DDQN | Acrobot-v1] Ep   70 | Score: -232.00 | Avg(10): -427.30 | Eps: 0.704\n",
      "[DDQN | Acrobot-v1] Ep   80 | Score: -380.00 | Avg(10): -457.50 | Eps: 0.670\n",
      "[DDQN | Acrobot-v1] Ep   90 | Score: -232.00 | Avg(10): -316.80 | Eps: 0.637\n",
      "[DDQN | Acrobot-v1] Ep  100 | Score: -169.00 | Avg(10): -362.40 | Eps: 0.606\n",
      "[DDQN | Acrobot-v1] Ep  110 | Score: -392.00 | Avg(10): -283.30 | Eps: 0.576\n",
      "[DDQN | Acrobot-v1] Ep  120 | Score: -173.00 | Avg(10): -270.40 | Eps: 0.548\n",
      "[DDQN | Acrobot-v1] Ep  130 | Score: -151.00 | Avg(10): -204.90 | Eps: 0.521\n",
      "[DDQN | Acrobot-v1] Ep  140 | Score: -255.00 | Avg(10): -194.50 | Eps: 0.496\n",
      "[DDQN | Acrobot-v1] Ep  150 | Score: -264.00 | Avg(10): -183.90 | Eps: 0.471\n",
      "[DDQN | Acrobot-v1] Ep  160 | Score: -129.00 | Avg(10): -186.00 | Eps: 0.448\n",
      "[DDQN | Acrobot-v1] Ep  170 | Score: -133.00 | Avg(10): -156.40 | Eps: 0.427\n",
      "[DDQN | Acrobot-v1] Ep  180 | Score: -237.00 | Avg(10): -158.60 | Eps: 0.406\n",
      "[DDQN | Acrobot-v1] Ep  190 | Score: -111.00 | Avg(10): -146.10 | Eps: 0.386\n",
      "[DDQN | Acrobot-v1] Ep  200 | Score: -165.00 | Avg(10): -150.10 | Eps: 0.367\n",
      "[DDQN | Acrobot-v1] Ep  210 | Score: -112.00 | Avg(10): -132.70 | Eps: 0.349\n",
      "[DDQN | Acrobot-v1] Ep  220 | Score: -165.00 | Avg(10): -143.40 | Eps: 0.332\n",
      "[DDQN | Acrobot-v1] Ep  230 | Score: -126.00 | Avg(10): -140.20 | Eps: 0.316\n",
      "[DDQN | Acrobot-v1] Ep  240 | Score: -134.00 | Avg(10): -124.60 | Eps: 0.300\n",
      "[DDQN | Acrobot-v1] Ep  250 | Score: -147.00 | Avg(10): -134.60 | Eps: 0.286\n",
      "[DDQN | Acrobot-v1] Ep  260 | Score: -103.00 | Avg(10): -144.20 | Eps: 0.272\n",
      "[DDQN | Acrobot-v1] Ep  270 | Score: -135.00 | Avg(10): -116.70 | Eps: 0.258\n",
      "[DDQN | Acrobot-v1] Ep  280 | Score: -112.00 | Avg(10): -120.30 | Eps: 0.246\n",
      "[DDQN | Acrobot-v1] Ep  290 | Score: -180.00 | Avg(10): -122.30 | Eps: 0.234\n",
      "[DDQN | Acrobot-v1] Ep  300 | Score:  -83.00 | Avg(10): -130.80 | Eps: 0.222\n",
      "[DDQN | Acrobot-v1] Ep  310 | Score: -103.00 | Avg(10): -110.80 | Eps: 0.211\n",
      "[DDQN | Acrobot-v1] Ep  320 | Score: -101.00 | Avg(10): -104.10 | Eps: 0.201\n",
      "[DDQN | Acrobot-v1] Ep  330 | Score: -110.00 | Avg(10): -131.10 | Eps: 0.191\n",
      "[DDQN | Acrobot-v1] Ep  340 | Score: -122.00 | Avg(10): -122.50 | Eps: 0.182\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: SELECT ENVIRONMENT + ALGORITHM AND RUN\n",
    "\n",
    "# Choose environment: \"CartPole-v1\" or \"Acrobot-v1\"\n",
    "ENV_NAME = \"Acrobot-v1\"   # or \"Acrobot-v1\"\n",
    "\n",
    "# Choose algorithm: False = DQN, True = DDQN\n",
    "USE_DDQN = True            # set to False to run plain DQN\n",
    "\n",
    "# Training settings\n",
    "NUM_EPISODES = 800   # you can raise this for Acrobot\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "agent, scores, moving_avgs = train_agent(\n",
    "    env_name=ENV_NAME,\n",
    "    use_ddqn=USE_DDQN,\n",
    "    num_episodes=NUM_EPISODES,\n",
    "    max_steps_per_episode=None,   # use default per env\n",
    "    solved_score=None,            # use default per env\n",
    "    print_every=PRINT_EVERY,\n",
    ")\n",
    "\n",
    "algo_name = \"DDQN\" if USE_DDQN else \"DQN\"\n",
    "gif_name = f\"V4-{algo_name}-{ENV_NAME}.gif\"\n",
    "\n",
    "make_gif(\n",
    "    agent=agent,\n",
    "    env_name=ENV_NAME,\n",
    "    filename=gif_name,\n",
    "    max_steps_per_episode=None,\n",
    "    fps=30,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
