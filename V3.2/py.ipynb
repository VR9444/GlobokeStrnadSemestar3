{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9692a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, utils as vutils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "# ---- Device: CUDA -> MPS (Apple Silicon) -> CPU ----\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# ---- Paths (TUKAJ POPRAVI, če imaš dataset drugje) ----\n",
    "# Ta pot mora kazati na folder, ki vsebuje trainA/, trainB/, testA/, testB/\n",
    "DATA_ROOT = Path(\"./apple2orange\")  # npr: Path(\"/Users/viktor/Downloads/apple2orange\")\n",
    "\n",
    "OUT_DIR = Path(\"./v3_2_outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Hyperparametri ----\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 1        # CycleGAN klasika: batch_size = 1\n",
    "NUM_EPOCHS = 15       # minimalno po navodilih (lahko daš več)\n",
    "LR = 2e-4\n",
    "\n",
    "LAMBDA_CYCLE = 10.0   # weight za cycle loss\n",
    "LAMBDA_ID = 5.0       # weight za identity loss (ponavadi 0.5 * LAMBDA_CYCLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc290d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainA (apples):   995 images\n",
      "TrainB (oranges):  1019 images\n",
      "TestA (apples):    266 images\n",
      "TestB (oranges):   248 images\n"
     ]
    }
   ],
   "source": [
    "# Transformacije (train + test)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE * 1.12)),\n",
    "    transforms.RandomCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                         (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                         (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "class SingleFolderDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.folder = Path(folder)\n",
    "        self.transform = transform\n",
    "        self.paths = list(self.folder.glob(\"*.*\"))\n",
    "        self.paths = [p for p in self.paths\n",
    "                      if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "trainA_dir = DATA_ROOT / \"trainA\"  # jabolka (X)\n",
    "trainB_dir = DATA_ROOT / \"trainB\"  # pomaranče (Y)\n",
    "testA_dir  = DATA_ROOT / \"testA\"\n",
    "testB_dir  = DATA_ROOT / \"testB\"\n",
    "\n",
    "trainA_ds = SingleFolderDataset(trainA_dir, transform_train)\n",
    "trainB_ds = SingleFolderDataset(trainB_dir, transform_train)\n",
    "testA_ds  = SingleFolderDataset(testA_dir,  transform_test)\n",
    "testB_ds  = SingleFolderDataset(testB_dir,  transform_test)\n",
    "\n",
    "trainA_loader = DataLoader(trainA_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                           num_workers=2, pin_memory=True)\n",
    "trainB_loader = DataLoader(trainB_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                           num_workers=2, pin_memory=True)\n",
    "\n",
    "testA_loader = DataLoader(testA_ds, batch_size=1, shuffle=True)\n",
    "testB_loader = DataLoader(testB_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "print(f\"TrainA (apples):   {len(trainA_ds)} images\")\n",
    "print(f\"TrainB (oranges):  {len(trainB_ds)} images\")\n",
    "print(f\"TestA (apples):    {len(testA_ds)} images\")\n",
    "print(f\"TestB (oranges):   {len(testB_ds)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d6d404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT: /Users/viktorrackov/Desktop/Feri/3.semestar/Globoke/V3.2/apple2orange\n",
      "  trainA_dir: apple2orange/trainA exists: True\n",
      "  trainB_dir: apple2orange/trainB exists: True\n",
      "  testA_dir : apple2orange/testA exists: True\n",
      "  testB_dir : apple2orange/testB exists: True\n",
      "TrainA (apples):   995 images\n",
      "TrainB (oranges):  1019 images\n",
      "TestA (apples):    266 images\n",
      "TestB (oranges):   248 images\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Dataset apple2orange – pričakovana struktura:\n",
    "# \n",
    "# DATA_ROOT/\n",
    "#   trainA/  (jabolka)\n",
    "#   trainB/  (pomaranče)\n",
    "#   testA/\n",
    "#   testB/\n",
    "\n",
    "# %%\n",
    "from PIL import Image\n",
    "\n",
    "# Transformacije (train + test)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE * 1.12)),\n",
    "    transforms.RandomCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                         (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                         (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "class SingleFolderDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset, ki vzame vse .jpg/.jpeg/.png slike iz ene mape.\n",
    "    \"\"\"\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.folder = Path(folder)\n",
    "        self.transform = transform\n",
    "        if self.folder.exists():\n",
    "            self.paths = [p for p in self.folder.glob(\"*.*\")\n",
    "                          if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]]\n",
    "        else:\n",
    "            self.paths = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "# Mape\n",
    "trainA_dir = DATA_ROOT / \"trainA\"  # jabolka (X)\n",
    "trainB_dir = DATA_ROOT / \"trainB\"  # pomaranče (Y)\n",
    "testA_dir  = DATA_ROOT / \"testA\"\n",
    "testB_dir  = DATA_ROOT / \"testB\"\n",
    "\n",
    "print(\"DATA_ROOT:\", DATA_ROOT.resolve())\n",
    "print(\"  trainA_dir:\", trainA_dir, \"exists:\", trainA_dir.exists())\n",
    "print(\"  trainB_dir:\", trainB_dir, \"exists:\", trainB_dir.exists())\n",
    "print(\"  testA_dir :\", testA_dir,  \"exists:\", testA_dir.exists())\n",
    "print(\"  testB_dir :\", testB_dir,  \"exists:\", testB_dir.exists())\n",
    "\n",
    "# Datasets\n",
    "trainA_ds = SingleFolderDataset(trainA_dir, transform_train)\n",
    "trainB_ds = SingleFolderDataset(trainB_dir, transform_train)\n",
    "testA_ds  = SingleFolderDataset(testA_dir,  transform_test)\n",
    "testB_ds  = SingleFolderDataset(testB_dir,  transform_test)\n",
    "\n",
    "print(f\"TrainA (apples):   {len(trainA_ds)} images\")\n",
    "print(f\"TrainB (oranges):  {len(trainB_ds)} images\")\n",
    "print(f\"TestA (apples):    {len(testA_ds)} images\")\n",
    "print(f\"TestB (oranges):   {len(testB_ds)} images\")\n",
    "\n",
    "# Če ni podatkov, takoj fail, da veš, da je treba popravit pot/dataset\n",
    "if len(trainA_ds) == 0 or len(trainB_ds) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"Dataset je prazen!\\n\"\n",
    "        f\" -> trainA ima {len(trainA_ds)} slik\\n\"\n",
    "        f\" -> trainB ima {len(trainB_ds)} slik\\n\"\n",
    "        \"Preveri DATA_ROOT in strukturo map: DATA_ROOT/trainA/*.jpg, DATA_ROOT/trainB/*.jpg\"\n",
    "    )\n",
    "\n",
    "# DataLoaderji (num_workers=0 = varno na macOS)\n",
    "trainA_loader = DataLoader(trainA_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                           num_workers=0, pin_memory=True)\n",
    "trainB_loader = DataLoader(trainB_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                           num_workers=0, pin_memory=True)\n",
    "\n",
    "testA_loader = DataLoader(testA_ds, batch_size=1, shuffle=True) if len(testA_ds) > 0 else None\n",
    "testB_loader = DataLoader(testB_ds, batch_size=1, shuffle=True) if len(testB_ds) > 0 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f80e1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT: /Users/viktorrackov/Desktop/Feri/3.semestar/Globoke/V3.2/apple2orange\n",
      "  trainA_dir: apple2orange/trainA exists: True\n",
      "  trainB_dir: apple2orange/trainB exists: True\n",
      "  testA_dir : apple2orange/testA exists: True\n",
      "  testB_dir : apple2orange/testB exists: True\n",
      "TrainA (apples):   995 images\n",
      "TrainB (oranges):  1019 images\n",
      "TestA (apples):    266 images\n",
      "TestB (oranges):   248 images\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Dataset apple2orange – pričakovana struktura:\n",
    "# \n",
    "# DATA_ROOT/\n",
    "#   trainA/  (jabolka)\n",
    "#   trainB/  (pomaranče)\n",
    "#   testA/\n",
    "#   testB/\n",
    "\n",
    "# %%\n",
    "from PIL import Image\n",
    "\n",
    "# Transformacije (train + test)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE * 1.12)),\n",
    "    transforms.RandomCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                         (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                         (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "class SingleFolderDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset, ki vzame vse .jpg/.jpeg/.png slike iz ene mape.\n",
    "    \"\"\"\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.folder = Path(folder)\n",
    "        self.transform = transform\n",
    "        if self.folder.exists():\n",
    "            self.paths = [p for p in self.folder.glob(\"*.*\")\n",
    "                          if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]]\n",
    "        else:\n",
    "            self.paths = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "# Mape\n",
    "trainA_dir = DATA_ROOT / \"trainA\"  # jabolka (X)\n",
    "trainB_dir = DATA_ROOT / \"trainB\"  # pomaranče (Y)\n",
    "testA_dir  = DATA_ROOT / \"testA\"\n",
    "testB_dir  = DATA_ROOT / \"testB\"\n",
    "\n",
    "print(\"DATA_ROOT:\", DATA_ROOT.resolve())\n",
    "print(\"  trainA_dir:\", trainA_dir, \"exists:\", trainA_dir.exists())\n",
    "print(\"  trainB_dir:\", trainB_dir, \"exists:\", trainB_dir.exists())\n",
    "print(\"  testA_dir :\", testA_dir,  \"exists:\", testA_dir.exists())\n",
    "print(\"  testB_dir :\", testB_dir,  \"exists:\", testB_dir.exists())\n",
    "\n",
    "# Datasets\n",
    "trainA_ds = SingleFolderDataset(trainA_dir, transform_train)\n",
    "trainB_ds = SingleFolderDataset(trainB_dir, transform_train)\n",
    "testA_ds  = SingleFolderDataset(testA_dir,  transform_test)\n",
    "testB_ds  = SingleFolderDataset(testB_dir,  transform_test)\n",
    "\n",
    "print(f\"TrainA (apples):   {len(trainA_ds)} images\")\n",
    "print(f\"TrainB (oranges):  {len(trainB_ds)} images\")\n",
    "print(f\"TestA (apples):    {len(testA_ds)} images\")\n",
    "print(f\"TestB (oranges):   {len(testB_ds)} images\")\n",
    "\n",
    "# Če ni podatkov, takoj fail, da veš, da je treba popravit pot/dataset\n",
    "if len(trainA_ds) == 0 or len(trainB_ds) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"Dataset je prazen!\\n\"\n",
    "        f\" -> trainA ima {len(trainA_ds)} slik\\n\"\n",
    "        f\" -> trainB ima {len(trainB_ds)} slik\\n\"\n",
    "        \"Preveri DATA_ROOT in strukturo map: DATA_ROOT/trainA/*.jpg, DATA_ROOT/trainB/*.jpg\"\n",
    "    )\n",
    "\n",
    "# DataLoaderji (num_workers=0 = varno na macOS)\n",
    "trainA_loader = DataLoader(trainA_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                           num_workers=0, pin_memory=True)\n",
    "trainB_loader = DataLoader(trainB_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                           num_workers=0, pin_memory=True)\n",
    "\n",
    "testA_loader = DataLoader(testA_ds, batch_size=1, shuffle=True) if len(testA_ds) > 0 else None\n",
    "testB_loader = DataLoader(testB_ds, batch_size=1, shuffle=True) if len(testB_ds) > 0 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee25dc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models initialized on mps\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Modeli (generatorji + diskriminatorji), inicializacija uteži,\n",
    "# loss funkcije, optimizerji in helper funkcije.\n",
    "\n",
    "# %%\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "\n",
    "# ---- ResNet blok ----\n",
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet blok: Conv - IN - ReLU - Conv - IN + skip.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "# ---- Generator ----\n",
    "class ResnetGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    CycleGAN generator (X -> Y ali Y -> X).\n",
    "    c7s1-64, d128, d256, R256xN, u128, u64, c7s1-3\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nc=3, output_nc=3, n_filters=64, n_blocks=6):\n",
    "        super().__init__()\n",
    "\n",
    "        model = []\n",
    "\n",
    "        # c7s1-64\n",
    "        model += [\n",
    "            nn.Conv2d(input_nc, n_filters, kernel_size=7, stride=1, padding=3, bias=False),\n",
    "            nn.InstanceNorm2d(n_filters),\n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "\n",
    "        # downsampling: d128, d256\n",
    "        in_c = n_filters\n",
    "        out_c = in_c * 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_c, out_c, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_c),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "            in_c = out_c\n",
    "            out_c *= 2\n",
    "\n",
    "        # ResNet bloki\n",
    "        for _ in range(n_blocks):\n",
    "            model.append(ResnetBlock(in_c))\n",
    "\n",
    "        # upsampling: u128, u64\n",
    "        out_c = in_c // 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_c, out_c, kernel_size=3, stride=2,\n",
    "                                   padding=1, output_padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_c),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "            in_c = out_c\n",
    "            out_c //= 2\n",
    "\n",
    "        # final c7s1-3 + Tanh\n",
    "        model += [\n",
    "            nn.Conv2d(in_c, output_nc, kernel_size=7, stride=1, padding=3),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ---- PatchGAN diskriminator ----\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchGAN diskriminator (70x70).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_nc=3, n_filters=64):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # C64 (brez norm)\n",
    "        layers += [\n",
    "            nn.Conv2d(input_nc, n_filters, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        # C128, C256, C512\n",
    "        in_c = n_filters\n",
    "        out_c = in_c * 2\n",
    "        for _ in range(3):\n",
    "            layers += [\n",
    "                nn.Conv2d(in_c, out_c, kernel_size=4,\n",
    "                          stride=2 if out_c <= 256 else 1,\n",
    "                          padding=1, bias=False),\n",
    "                nn.InstanceNorm2d(out_c),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "            in_c = out_c\n",
    "            out_c = min(out_c * 2, 512)\n",
    "\n",
    "        # izhod: 1 kanal\n",
    "        layers += [\n",
    "            nn.Conv2d(in_c, 1, kernel_size=4, stride=1, padding=1)\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ---- Inic. uteži ----\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif isinstance(m, nn.InstanceNorm2d):\n",
    "        if m.weight is not None:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "# ---- Ustvarimo modele ----\n",
    "G_X2Y = ResnetGenerator(input_nc=3, output_nc=3, n_filters=64, n_blocks=6).to(DEVICE)\n",
    "G_Y2X = ResnetGenerator(input_nc=3, output_nc=3, n_filters=64, n_blocks=6).to(DEVICE)\n",
    "\n",
    "D_X = PatchDiscriminator(input_nc=3, n_filters=64).to(DEVICE)  # real vs fake apples\n",
    "D_Y = PatchDiscriminator(input_nc=3, n_filters=64).to(DEVICE)  # real vs fake oranges\n",
    "\n",
    "G_X2Y.apply(init_weights)\n",
    "G_Y2X.apply(init_weights)\n",
    "D_X.apply(init_weights)\n",
    "D_Y.apply(init_weights)\n",
    "\n",
    "print(\"Models initialized on\", DEVICE)\n",
    "\n",
    "# ---- Loss funkcije ----\n",
    "criterion_GAN = nn.MSELoss()    # LSGAN\n",
    "criterion_cycle = nn.L1Loss()   # cycle loss\n",
    "criterion_identity = nn.L1Loss()# identity loss\n",
    "\n",
    "# ---- Optimizatorji ----\n",
    "optimizer_G = optim.Adam(\n",
    "    itertools.chain(G_X2Y.parameters(), G_Y2X.parameters()),\n",
    "    lr=LR, betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_D_X = optim.Adam(D_X.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "optimizer_D_Y = optim.Adam(D_Y.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "\n",
    "# ---- Helperji ----\n",
    "def set_requires_grad(nets, requires_grad=False):\n",
    "    \"\"\"\n",
    "    Omogoči/onemogoči gradiente za podane mreže.\n",
    "    \"\"\"\n",
    "    if not isinstance(nets, list):\n",
    "        nets = [nets]\n",
    "    for net in nets:\n",
    "        for p in net.parameters():\n",
    "            p.requires_grad = requires_grad\n",
    "\n",
    "def denormalize(tensor):\n",
    "    \"\"\"\n",
    "    Pretvori iz [-1, 1] v [0, 1] za prikaz/shranjevanje.\n",
    "    \"\"\"\n",
    "    out = tensor * 0.5 + 0.5\n",
    "    return torch.clamp(out, 0.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "827d8faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Loss funkcije:\n",
    "# - GAN loss: MSE (LSGAN)\n",
    "# - Cycle loss: L1\n",
    "# - Identity loss: L1\n",
    "# \n",
    "# Optimizatorji: Adam za G in oba D.\n",
    "\n",
    "# %%\n",
    "# GAN loss (LSGAN)\n",
    "criterion_GAN = nn.MSELoss()\n",
    "# Cycle & identity loss\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "# Optimizatorji\n",
    "optimizer_G = optim.Adam(\n",
    "    itertools.chain(G_X2Y.parameters(), G_Y2X.parameters()),\n",
    "    lr=LR, betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_D_X = optim.Adam(D_X.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "optimizer_D_Y = optim.Adam(D_Y.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "\n",
    "def set_requires_grad(nets, requires_grad=False):\n",
    "    \"\"\"\n",
    "    Omogoči/onemogoči gradiente za podane mreže.\n",
    "    \"\"\"\n",
    "    if not isinstance(nets, list):\n",
    "        nets = [nets]\n",
    "    for net in nets:\n",
    "        for p in net.parameters():\n",
    "            p.requires_grad = requires_grad\n",
    "\n",
    "def denormalize(tensor):\n",
    "    \"\"\"\n",
    "    Transformacija iz [-1, 1] v [0, 1] za prikaz/shranjevanje.\n",
    "    \"\"\"\n",
    "    out = tensor * 0.5 + 0.5\n",
    "    return torch.clamp(out, 0.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e26271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/viktorrackov/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  Batch 10  G: 4.3877  DX: 0.3734  DY: 0.2205\n",
      "Epoch 1/15  Batch 20  G: 6.4306  DX: 0.2147  DY: 0.3843\n",
      "Epoch 1/15  Batch 30  G: 12.2935  DX: 0.2412  DY: 0.3707\n",
      "Epoch 1/15  Batch 40  G: 6.2537  DX: 0.2920  DY: 0.1939\n",
      "Epoch 1/15  Batch 50  G: 9.7201  DX: 0.3083  DY: 0.3191\n",
      "Epoch 1/15  Batch 60  G: 7.7409  DX: 0.1982  DY: 0.3684\n",
      "Epoch 1/15  Batch 70  G: 9.1377  DX: 0.3423  DY: 0.2667\n",
      "Epoch 1/15  Batch 80  G: 10.0802  DX: 0.1796  DY: 0.1751\n",
      "Epoch 1/15  Batch 90  G: 7.9856  DX: 0.2260  DY: 0.3025\n",
      "Epoch 1/15  Batch 100  G: 10.6934  DX: 0.2468  DY: 0.2351\n",
      "Epoch 1/15  Batch 110  G: 7.8437  DX: 0.3939  DY: 0.1672\n",
      "Epoch 1/15  Batch 120  G: 5.9666  DX: 0.1889  DY: 0.3992\n",
      "Epoch 1/15  Batch 130  G: 6.6251  DX: 0.3070  DY: 0.1708\n",
      "Epoch 1/15  Batch 140  G: 7.7302  DX: 0.2040  DY: 0.1198\n",
      "Epoch 1/15  Batch 150  G: 7.5809  DX: 0.2122  DY: 0.1753\n",
      "Epoch 1/15  Batch 160  G: 7.4866  DX: 0.3983  DY: 0.2820\n",
      "Epoch 1/15  Batch 170  G: 9.1054  DX: 0.4383  DY: 0.5241\n",
      "Epoch 1/15  Batch 180  G: 7.6075  DX: 0.2639  DY: 0.3241\n",
      "Epoch 1/15  Batch 190  G: 10.3629  DX: 0.2135  DY: 0.3436\n",
      "Epoch 1/15  Batch 200  G: 8.1027  DX: 0.2670  DY: 0.4395\n",
      "Epoch 1/15  Batch 210  G: 5.1066  DX: 0.1792  DY: 0.1570\n",
      "Epoch 1/15  Batch 220  G: 8.1334  DX: 0.1854  DY: 0.2867\n",
      "Epoch 1/15  Batch 230  G: 9.8733  DX: 0.2397  DY: 0.2420\n",
      "Epoch 1/15  Batch 240  G: 6.9699  DX: 0.2056  DY: 0.0813\n",
      "Epoch 1/15  Batch 250  G: 6.9991  DX: 0.3091  DY: 0.1742\n",
      "Epoch 1/15  Batch 260  G: 9.0980  DX: 0.4151  DY: 0.2217\n",
      "Epoch 1/15  Batch 270  G: 9.4522  DX: 0.1554  DY: 0.2987\n",
      "Epoch 1/15  Batch 280  G: 7.3021  DX: 0.3224  DY: 0.1911\n",
      "Epoch 1/15  Batch 290  G: 5.8864  DX: 0.4270  DY: 0.2165\n",
      "Epoch 1/15  Batch 300  G: 10.3784  DX: 0.1636  DY: 0.2356\n",
      "Epoch 1/15  Batch 310  G: 7.6751  DX: 0.1818  DY: 0.2513\n",
      "Epoch 1/15  Batch 320  G: 10.4905  DX: 0.1852  DY: 0.2445\n",
      "Epoch 1/15  Batch 330  G: 4.8565  DX: 0.1463  DY: 0.1489\n",
      "Epoch 1/15  Batch 340  G: 6.1464  DX: 0.4698  DY: 0.3799\n",
      "Epoch 1/15  Batch 350  G: 7.3863  DX: 0.2512  DY: 0.2086\n",
      "Epoch 1/15  Batch 360  G: 8.3993  DX: 0.1492  DY: 0.1355\n",
      "Epoch 1/15  Batch 370  G: 7.7140  DX: 0.3498  DY: 0.1872\n",
      "Epoch 1/15  Batch 380  G: 8.5705  DX: 0.3835  DY: 0.2196\n",
      "Epoch 1/15  Batch 390  G: 4.8563  DX: 0.2453  DY: 0.3035\n",
      "Epoch 1/15  Batch 400  G: 8.4371  DX: 0.1588  DY: 0.2564\n",
      "Epoch 1/15  Batch 410  G: 8.3396  DX: 0.2537  DY: 0.1105\n",
      "Epoch 1/15  Batch 420  G: 7.0297  DX: 0.2772  DY: 0.1867\n",
      "Epoch 1/15  Batch 430  G: 7.5889  DX: 0.2975  DY: 0.1794\n",
      "Epoch 1/15  Batch 440  G: 7.3651  DX: 0.2720  DY: 0.2764\n",
      "Epoch 1/15  Batch 450  G: 8.5317  DX: 0.2864  DY: 0.3882\n",
      "Epoch 1/15  Batch 460  G: 10.9391  DX: 0.2732  DY: 0.2313\n",
      "Epoch 1/15  Batch 470  G: 7.6974  DX: 0.2848  DY: 0.2775\n",
      "Epoch 1/15  Batch 480  G: 6.6013  DX: 0.2045  DY: 0.1323\n",
      "Epoch 1/15  Batch 490  G: 6.6724  DX: 0.2573  DY: 0.2700\n",
      "Epoch 1/15  Batch 500  G: 6.6258  DX: 0.2378  DY: 0.2992\n",
      "Epoch 1/15  Batch 510  G: 6.1627  DX: 0.2995  DY: 0.1232\n",
      "Epoch 1/15  Batch 520  G: 7.7734  DX: 0.1152  DY: 0.2615\n",
      "Epoch 1/15  Batch 530  G: 6.9387  DX: 0.3180  DY: 0.1838\n",
      "Epoch 1/15  Batch 540  G: 6.0376  DX: 0.2481  DY: 0.2517\n",
      "Epoch 1/15  Batch 550  G: 5.9324  DX: 0.2735  DY: 0.2752\n",
      "Epoch 1/15  Batch 560  G: 9.8082  DX: 0.2152  DY: 0.3238\n",
      "Epoch 1/15  Batch 570  G: 7.6861  DX: 0.2504  DY: 0.1029\n",
      "Epoch 1/15  Batch 580  G: 7.2565  DX: 0.3542  DY: 0.2437\n",
      "Epoch 1/15  Batch 590  G: 7.1445  DX: 0.6601  DY: 0.3418\n",
      "Epoch 1/15  Batch 600  G: 7.4309  DX: 0.2043  DY: 0.1787\n",
      "Epoch 1/15  Batch 610  G: 6.0866  DX: 0.2113  DY: 0.2656\n",
      "Epoch 1/15  Batch 620  G: 8.6639  DX: 0.2830  DY: 0.2348\n",
      "Epoch 1/15  Batch 630  G: 8.1919  DX: 0.2798  DY: 0.2954\n",
      "Epoch 1/15  Batch 640  G: 7.2669  DX: 0.2902  DY: 0.2253\n",
      "Epoch 1/15  Batch 650  G: 6.2634  DX: 0.2178  DY: 0.2351\n",
      "Epoch 1/15  Batch 660  G: 6.6400  DX: 0.2741  DY: 0.1651\n",
      "Epoch 1/15  Batch 670  G: 8.9685  DX: 0.1195  DY: 0.2619\n",
      "Epoch 1/15  Batch 680  G: 7.3331  DX: 0.2771  DY: 0.3271\n",
      "Epoch 1/15  Batch 690  G: 8.7239  DX: 0.1379  DY: 0.4402\n",
      "Epoch 1/15  Batch 700  G: 9.6753  DX: 0.3210  DY: 0.2960\n",
      "Epoch 1/15  Batch 710  G: 6.7457  DX: 0.2538  DY: 0.1252\n",
      "Epoch 1/15  Batch 720  G: 7.1601  DX: 0.2341  DY: 0.2476\n",
      "Epoch 1/15  Batch 730  G: 6.2346  DX: 0.1152  DY: 0.0962\n",
      "Epoch 1/15  Batch 740  G: 9.7044  DX: 0.3555  DY: 0.2403\n",
      "Epoch 1/15  Batch 750  G: 6.7308  DX: 0.2995  DY: 0.2865\n",
      "Epoch 1/15  Batch 760  G: 6.6795  DX: 0.2558  DY: 0.2158\n",
      "Epoch 1/15  Batch 770  G: 5.0225  DX: 0.1161  DY: 0.2587\n",
      "Epoch 1/15  Batch 780  G: 4.9160  DX: 0.3059  DY: 0.3142\n",
      "Epoch 1/15  Batch 790  G: 5.5487  DX: 0.2424  DY: 0.2471\n",
      "Epoch 1/15  Batch 800  G: 5.7927  DX: 0.2793  DY: 0.3535\n",
      "Epoch 1/15  Batch 810  G: 9.8512  DX: 0.2325  DY: 0.2177\n",
      "Epoch 1/15  Batch 820  G: 6.1033  DX: 0.3515  DY: 0.2158\n",
      "Epoch 1/15  Batch 830  G: 5.4717  DX: 0.4003  DY: 0.2400\n",
      "Epoch 1/15  Batch 840  G: 7.5297  DX: 0.2310  DY: 0.3252\n",
      "Epoch 1/15  Batch 850  G: 7.7068  DX: 0.1443  DY: 0.3583\n",
      "Epoch 1/15  Batch 860  G: 5.9500  DX: 0.3643  DY: 0.4371\n",
      "Epoch 1/15  Batch 870  G: 8.5769  DX: 0.2419  DY: 0.2750\n",
      "Epoch 1/15  Batch 880  G: 5.9639  DX: 0.3319  DY: 0.2888\n",
      "Epoch 1/15  Batch 890  G: 6.8850  DX: 0.1424  DY: 0.3558\n",
      "Epoch 1/15  Batch 900  G: 8.0769  DX: 0.2360  DY: 0.2480\n",
      "Epoch 1/15  Batch 910  G: 4.4521  DX: 0.2669  DY: 0.3381\n",
      "Epoch 1/15  Batch 920  G: 10.9299  DX: 0.2678  DY: 0.2436\n",
      "Epoch 1/15  Batch 930  G: 4.3653  DX: 0.3069  DY: 0.1475\n",
      "Epoch 1/15  Batch 940  G: 5.8948  DX: 0.2540  DY: 0.1472\n",
      "Epoch 1/15  Batch 950  G: 7.9762  DX: 0.2159  DY: 0.3176\n",
      "Epoch 1/15  Batch 960  G: 7.3066  DX: 0.3771  DY: 0.1426\n",
      "Epoch 1/15  Batch 970  G: 7.0609  DX: 0.2958  DY: 0.1799\n",
      "Epoch 1/15  Batch 980  G: 6.1849  DX: 0.2393  DY: 0.3641\n",
      "Epoch 1/15  Batch 990  G: 7.1386  DX: 0.2737  DY: 0.2939\n",
      "==> Epoch [1/15]  G_loss: 7.5165  D_X_loss: 0.2617  D_Y_loss: 0.2625\n",
      "Epoch 2/15  Batch 10  G: 5.9481  DX: 0.2360  DY: 0.2357\n",
      "Epoch 2/15  Batch 20  G: 6.5013  DX: 0.1493  DY: 0.2197\n",
      "Epoch 2/15  Batch 30  G: 6.8406  DX: 0.1819  DY: 0.1852\n",
      "Epoch 2/15  Batch 40  G: 8.8317  DX: 0.2238  DY: 0.1567\n",
      "Epoch 2/15  Batch 50  G: 6.9731  DX: 0.1837  DY: 0.2019\n",
      "Epoch 2/15  Batch 60  G: 7.7665  DX: 0.1883  DY: 0.2260\n",
      "Epoch 2/15  Batch 70  G: 8.6164  DX: 0.1939  DY: 0.1950\n",
      "Epoch 2/15  Batch 80  G: 8.8453  DX: 0.3571  DY: 0.1103\n",
      "Epoch 2/15  Batch 90  G: 5.5518  DX: 0.3227  DY: 0.2169\n",
      "Epoch 2/15  Batch 100  G: 6.9565  DX: 0.2565  DY: 0.3523\n",
      "Epoch 2/15  Batch 110  G: 6.9752  DX: 0.1809  DY: 0.2825\n",
      "Epoch 2/15  Batch 120  G: 5.0299  DX: 0.1397  DY: 0.2608\n",
      "Epoch 2/15  Batch 130  G: 5.6634  DX: 0.1216  DY: 0.2073\n",
      "Epoch 2/15  Batch 140  G: 5.9498  DX: 0.2639  DY: 0.2089\n",
      "Epoch 2/15  Batch 150  G: 7.2416  DX: 0.1965  DY: 0.3596\n",
      "Epoch 2/15  Batch 160  G: 7.5894  DX: 0.2218  DY: 0.2126\n",
      "Epoch 2/15  Batch 170  G: 5.7446  DX: 0.3113  DY: 0.2357\n",
      "Epoch 2/15  Batch 180  G: 6.5404  DX: 0.5308  DY: 0.1981\n",
      "Epoch 2/15  Batch 190  G: 7.5150  DX: 0.1608  DY: 0.1624\n",
      "Epoch 2/15  Batch 200  G: 6.4141  DX: 0.1828  DY: 0.1928\n",
      "Epoch 2/15  Batch 210  G: 7.3083  DX: 0.1550  DY: 0.2375\n",
      "Epoch 2/15  Batch 220  G: 7.1225  DX: 0.1000  DY: 0.3041\n",
      "Epoch 2/15  Batch 230  G: 4.4901  DX: 0.2259  DY: 0.3471\n",
      "Epoch 2/15  Batch 240  G: 8.3877  DX: 0.2333  DY: 0.2211\n",
      "Epoch 2/15  Batch 250  G: 8.0043  DX: 0.2278  DY: 0.3863\n",
      "Epoch 2/15  Batch 260  G: 6.6475  DX: 0.1565  DY: 0.1873\n",
      "Epoch 2/15  Batch 270  G: 6.2652  DX: 0.1884  DY: 0.0759\n",
      "Epoch 2/15  Batch 280  G: 3.9639  DX: 0.4793  DY: 0.3233\n",
      "Epoch 2/15  Batch 290  G: 6.7835  DX: 0.1895  DY: 0.2605\n",
      "Epoch 2/15  Batch 300  G: 5.8567  DX: 0.4598  DY: 0.3428\n",
      "Epoch 2/15  Batch 310  G: 6.3352  DX: 0.1876  DY: 0.0814\n",
      "Epoch 2/15  Batch 320  G: 3.3645  DX: 0.3859  DY: 0.2397\n",
      "Epoch 2/15  Batch 330  G: 7.1552  DX: 0.2138  DY: 0.2662\n",
      "Epoch 2/15  Batch 340  G: 6.9454  DX: 0.1454  DY: 0.3518\n",
      "Epoch 2/15  Batch 350  G: 6.8625  DX: 0.1366  DY: 0.2169\n",
      "Epoch 2/15  Batch 360  G: 5.4156  DX: 0.2031  DY: 0.2179\n",
      "Epoch 2/15  Batch 370  G: 6.4701  DX: 0.1630  DY: 0.1678\n",
      "Epoch 2/15  Batch 380  G: 6.0858  DX: 0.1999  DY: 0.2465\n",
      "Epoch 2/15  Batch 390  G: 5.4954  DX: 0.2140  DY: 0.2886\n",
      "Epoch 2/15  Batch 400  G: 4.5347  DX: 0.3322  DY: 0.3000\n",
      "Epoch 2/15  Batch 410  G: 6.3590  DX: 0.2021  DY: 0.3056\n",
      "Epoch 2/15  Batch 420  G: 7.7962  DX: 0.3790  DY: 0.2512\n",
      "Epoch 2/15  Batch 430  G: 6.0679  DX: 0.2565  DY: 0.3484\n",
      "Epoch 2/15  Batch 440  G: 5.7349  DX: 0.2446  DY: 0.1261\n",
      "Epoch 2/15  Batch 450  G: 7.1238  DX: 0.2519  DY: 0.1031\n",
      "Epoch 2/15  Batch 460  G: 7.2401  DX: 0.1121  DY: 0.1025\n",
      "Epoch 2/15  Batch 470  G: 6.7149  DX: 0.0882  DY: 0.1210\n",
      "Epoch 2/15  Batch 480  G: 5.1233  DX: 0.2572  DY: 0.0966\n",
      "Epoch 2/15  Batch 490  G: 9.7046  DX: 0.4462  DY: 0.5201\n",
      "Epoch 2/15  Batch 500  G: 4.6241  DX: 0.2295  DY: 0.1640\n",
      "Epoch 2/15  Batch 510  G: 5.5337  DX: 0.1247  DY: 0.2422\n",
      "Epoch 2/15  Batch 520  G: 6.0095  DX: 0.2831  DY: 0.4186\n",
      "Epoch 2/15  Batch 530  G: 10.8383  DX: 0.6675  DY: 0.3351\n",
      "Epoch 2/15  Batch 540  G: 7.0402  DX: 0.1461  DY: 0.2141\n",
      "Epoch 2/15  Batch 550  G: 7.7901  DX: 0.2134  DY: 0.0960\n",
      "Epoch 2/15  Batch 560  G: 6.6251  DX: 0.1852  DY: 0.5641\n",
      "Epoch 2/15  Batch 570  G: 7.3092  DX: 0.4210  DY: 0.3178\n",
      "Epoch 2/15  Batch 580  G: 5.0667  DX: 0.2487  DY: 0.1116\n",
      "Epoch 2/15  Batch 590  G: 4.7402  DX: 0.5458  DY: 0.4460\n",
      "Epoch 2/15  Batch 600  G: 5.7370  DX: 0.3255  DY: 0.3538\n",
      "Epoch 2/15  Batch 610  G: 5.4660  DX: 0.3289  DY: 0.2420\n",
      "Epoch 2/15  Batch 620  G: 5.8106  DX: 0.2201  DY: 0.3857\n",
      "Epoch 2/15  Batch 630  G: 8.5993  DX: 0.2726  DY: 0.3197\n",
      "Epoch 2/15  Batch 640  G: 7.7858  DX: 0.1993  DY: 0.2890\n",
      "Epoch 2/15  Batch 650  G: 5.5656  DX: 0.2009  DY: 0.2577\n",
      "Epoch 2/15  Batch 660  G: 6.4983  DX: 0.2697  DY: 0.1825\n",
      "Epoch 2/15  Batch 670  G: 8.3062  DX: 0.2720  DY: 0.3706\n",
      "Epoch 2/15  Batch 680  G: 5.1700  DX: 0.1727  DY: 0.2516\n",
      "Epoch 2/15  Batch 690  G: 5.5833  DX: 0.2765  DY: 0.1130\n",
      "Epoch 2/15  Batch 700  G: 6.3399  DX: 0.2525  DY: 0.3509\n",
      "Epoch 2/15  Batch 710  G: 6.8429  DX: 0.2299  DY: 0.2435\n",
      "Epoch 2/15  Batch 720  G: 10.1315  DX: 0.2635  DY: 0.2722\n",
      "Epoch 2/15  Batch 730  G: 5.6484  DX: 0.1778  DY: 0.2865\n",
      "Epoch 2/15  Batch 740  G: 5.3376  DX: 0.2015  DY: 0.1675\n",
      "Epoch 2/15  Batch 750  G: 5.3368  DX: 0.2696  DY: 0.3024\n",
      "Epoch 2/15  Batch 760  G: 7.0926  DX: 0.2617  DY: 0.1724\n",
      "Epoch 2/15  Batch 770  G: 6.6571  DX: 0.1626  DY: 0.2778\n",
      "Epoch 2/15  Batch 780  G: 5.6540  DX: 0.1494  DY: 0.2446\n",
      "Epoch 2/15  Batch 790  G: 5.8038  DX: 0.3303  DY: 0.2277\n",
      "Epoch 2/15  Batch 800  G: 8.0052  DX: 0.1363  DY: 0.3626\n",
      "Epoch 2/15  Batch 810  G: 7.8415  DX: 0.3756  DY: 0.2040\n",
      "Epoch 2/15  Batch 820  G: 6.6605  DX: 0.1919  DY: 0.1658\n",
      "Epoch 2/15  Batch 830  G: 5.9160  DX: 0.1487  DY: 0.2296\n",
      "Epoch 2/15  Batch 840  G: 5.5524  DX: 0.3363  DY: 0.2849\n",
      "Epoch 2/15  Batch 850  G: 5.0321  DX: 0.0781  DY: 0.1241\n",
      "Epoch 2/15  Batch 860  G: 3.6573  DX: 0.2109  DY: 0.1613\n",
      "Epoch 2/15  Batch 870  G: 6.2806  DX: 0.3183  DY: 0.2131\n",
      "Epoch 2/15  Batch 880  G: 10.4602  DX: 0.1449  DY: 0.2168\n",
      "Epoch 2/15  Batch 890  G: 6.7132  DX: 0.2884  DY: 0.3359\n",
      "Epoch 2/15  Batch 900  G: 9.4085  DX: 0.1517  DY: 0.2161\n",
      "Epoch 2/15  Batch 910  G: 5.4024  DX: 0.5071  DY: 0.4339\n",
      "Epoch 2/15  Batch 920  G: 6.2247  DX: 0.2236  DY: 0.2929\n",
      "Epoch 2/15  Batch 930  G: 6.3548  DX: 0.1188  DY: 0.1741\n",
      "Epoch 2/15  Batch 940  G: 6.0185  DX: 0.1362  DY: 0.2764\n",
      "Epoch 2/15  Batch 950  G: 6.1142  DX: 0.2147  DY: 0.1020\n",
      "Epoch 2/15  Batch 960  G: 4.8738  DX: 0.3199  DY: 0.2193\n",
      "Epoch 2/15  Batch 970  G: 8.8133  DX: 0.4138  DY: 0.4091\n",
      "Epoch 2/15  Batch 980  G: 6.9826  DX: 0.2344  DY: 0.1745\n",
      "Epoch 2/15  Batch 990  G: 9.4115  DX: 0.2063  DY: 0.2279\n",
      "==> Epoch [2/15]  G_loss: 6.5962  D_X_loss: 0.2509  D_Y_loss: 0.2506\n",
      "Epoch 3/15  Batch 10  G: 6.6891  DX: 0.1375  DY: 0.1185\n",
      "Epoch 3/15  Batch 20  G: 6.8563  DX: 0.1997  DY: 0.1355\n",
      "Epoch 3/15  Batch 30  G: 6.8475  DX: 0.2271  DY: 0.2962\n",
      "Epoch 3/15  Batch 40  G: 6.1627  DX: 0.2280  DY: 0.2038\n",
      "Epoch 3/15  Batch 50  G: 7.3772  DX: 0.2927  DY: 0.3523\n",
      "Epoch 3/15  Batch 60  G: 5.0038  DX: 0.1952  DY: 0.1163\n",
      "Epoch 3/15  Batch 70  G: 6.4647  DX: 0.2077  DY: 0.2319\n",
      "Epoch 3/15  Batch 80  G: 7.7051  DX: 0.2908  DY: 0.0998\n",
      "Epoch 3/15  Batch 90  G: 6.4538  DX: 0.3723  DY: 0.2118\n",
      "Epoch 3/15  Batch 100  G: 8.2143  DX: 0.4078  DY: 0.3810\n",
      "Epoch 3/15  Batch 110  G: 5.0501  DX: 0.6186  DY: 0.6507\n",
      "Epoch 3/15  Batch 120  G: 5.5242  DX: 0.3712  DY: 0.2199\n",
      "Epoch 3/15  Batch 130  G: 6.8344  DX: 0.1489  DY: 0.1972\n",
      "Epoch 3/15  Batch 140  G: 8.0873  DX: 0.3494  DY: 0.1726\n",
      "Epoch 3/15  Batch 150  G: 7.0228  DX: 0.2527  DY: 0.0819\n",
      "Epoch 3/15  Batch 160  G: 5.8132  DX: 0.2160  DY: 0.2145\n",
      "Epoch 3/15  Batch 170  G: 6.3705  DX: 0.1810  DY: 0.2017\n",
      "Epoch 3/15  Batch 180  G: 7.3844  DX: 0.3746  DY: 0.3846\n",
      "Epoch 3/15  Batch 190  G: 4.7524  DX: 0.1868  DY: 0.2549\n",
      "Epoch 3/15  Batch 200  G: 6.6139  DX: 0.1454  DY: 0.2080\n",
      "Epoch 3/15  Batch 210  G: 7.0791  DX: 0.4023  DY: 0.1504\n",
      "Epoch 3/15  Batch 220  G: 4.2518  DX: 0.6854  DY: 0.2163\n",
      "Epoch 3/15  Batch 230  G: 5.7832  DX: 0.1616  DY: 0.2728\n",
      "Epoch 3/15  Batch 240  G: 5.9357  DX: 0.3391  DY: 0.2953\n",
      "Epoch 3/15  Batch 250  G: 4.7599  DX: 0.4187  DY: 0.2182\n",
      "Epoch 3/15  Batch 260  G: 6.0633  DX: 0.2020  DY: 0.4271\n",
      "Epoch 3/15  Batch 270  G: 4.6730  DX: 0.1867  DY: 0.2422\n",
      "Epoch 3/15  Batch 280  G: 5.4331  DX: 0.3821  DY: 0.1755\n",
      "Epoch 3/15  Batch 290  G: 5.3080  DX: 0.1944  DY: 0.2341\n",
      "Epoch 3/15  Batch 300  G: 5.2675  DX: 0.1334  DY: 0.1285\n",
      "Epoch 3/15  Batch 310  G: 5.6719  DX: 0.3523  DY: 0.1434\n",
      "Epoch 3/15  Batch 320  G: 5.6483  DX: 0.2323  DY: 0.0803\n",
      "Epoch 3/15  Batch 330  G: 6.2978  DX: 0.2026  DY: 0.2239\n",
      "Epoch 3/15  Batch 340  G: 6.1912  DX: 0.2932  DY: 0.2297\n",
      "Epoch 3/15  Batch 350  G: 7.7807  DX: 0.0930  DY: 0.2934\n",
      "Epoch 3/15  Batch 360  G: 7.8927  DX: 0.2263  DY: 0.2282\n",
      "Epoch 3/15  Batch 370  G: 7.1559  DX: 0.2406  DY: 0.2088\n",
      "Epoch 3/15  Batch 380  G: 4.6361  DX: 0.1107  DY: 0.2356\n",
      "Epoch 3/15  Batch 390  G: 4.2831  DX: 0.1426  DY: 0.2338\n",
      "Epoch 3/15  Batch 400  G: 7.1919  DX: 0.1718  DY: 0.2043\n",
      "Epoch 3/15  Batch 410  G: 7.5449  DX: 0.1219  DY: 0.1380\n",
      "Epoch 3/15  Batch 420  G: 6.4194  DX: 0.1669  DY: 0.2026\n",
      "Epoch 3/15  Batch 430  G: 7.1635  DX: 0.1012  DY: 0.1554\n",
      "Epoch 3/15  Batch 440  G: 5.6137  DX: 0.2498  DY: 0.2947\n",
      "Epoch 3/15  Batch 450  G: 6.1748  DX: 0.3399  DY: 0.3605\n",
      "Epoch 3/15  Batch 460  G: 8.0289  DX: 0.2086  DY: 0.4134\n",
      "Epoch 3/15  Batch 470  G: 5.7321  DX: 0.2734  DY: 0.1532\n",
      "Epoch 3/15  Batch 480  G: 3.9270  DX: 0.2436  DY: 0.3167\n",
      "Epoch 3/15  Batch 490  G: 5.5853  DX: 0.2160  DY: 0.1796\n",
      "Epoch 3/15  Batch 500  G: 5.1394  DX: 0.3779  DY: 0.1399\n",
      "Epoch 3/15  Batch 510  G: 6.5026  DX: 0.1383  DY: 0.4060\n",
      "Epoch 3/15  Batch 520  G: 9.0009  DX: 0.5722  DY: 0.2365\n",
      "Epoch 3/15  Batch 530  G: 5.5892  DX: 0.0904  DY: 0.0815\n",
      "Epoch 3/15  Batch 540  G: 5.8675  DX: 0.1527  DY: 0.1520\n",
      "Epoch 3/15  Batch 550  G: 5.6940  DX: 0.4297  DY: 0.3422\n",
      "Epoch 3/15  Batch 560  G: 6.2041  DX: 0.4937  DY: 0.2424\n",
      "Epoch 3/15  Batch 570  G: 6.9218  DX: 0.2207  DY: 0.2626\n",
      "Epoch 3/15  Batch 580  G: 3.3401  DX: 0.3219  DY: 0.3269\n",
      "Epoch 3/15  Batch 590  G: 3.8602  DX: 0.2926  DY: 0.2522\n",
      "Epoch 3/15  Batch 600  G: 5.8476  DX: 0.1210  DY: 0.4034\n",
      "Epoch 3/15  Batch 610  G: 5.9975  DX: 0.2942  DY: 0.3892\n",
      "Epoch 3/15  Batch 620  G: 7.1091  DX: 0.0888  DY: 0.2138\n",
      "Epoch 3/15  Batch 630  G: 8.1090  DX: 0.1935  DY: 0.0635\n",
      "Epoch 3/15  Batch 640  G: 8.0822  DX: 0.4109  DY: 0.2723\n",
      "Epoch 3/15  Batch 650  G: 4.9842  DX: 0.4738  DY: 0.2956\n",
      "Epoch 3/15  Batch 660  G: 6.4300  DX: 0.2300  DY: 0.1974\n",
      "Epoch 3/15  Batch 670  G: 7.0092  DX: 0.1745  DY: 0.2523\n",
      "Epoch 3/15  Batch 680  G: 4.4373  DX: 0.3508  DY: 0.2623\n",
      "Epoch 3/15  Batch 690  G: 4.7707  DX: 0.1944  DY: 0.2686\n",
      "Epoch 3/15  Batch 700  G: 3.7580  DX: 0.7267  DY: 0.2792\n",
      "Epoch 3/15  Batch 710  G: 3.8644  DX: 0.1699  DY: 0.3161\n",
      "Epoch 3/15  Batch 720  G: 5.5061  DX: 0.2694  DY: 0.2481\n",
      "Epoch 3/15  Batch 730  G: 4.8062  DX: 0.2188  DY: 0.2464\n",
      "Epoch 3/15  Batch 740  G: 6.7291  DX: 0.2708  DY: 0.2789\n",
      "Epoch 3/15  Batch 750  G: 7.9232  DX: 0.2139  DY: 0.1912\n",
      "Epoch 3/15  Batch 760  G: 3.7314  DX: 0.3039  DY: 0.2500\n",
      "Epoch 3/15  Batch 770  G: 6.4473  DX: 0.2723  DY: 0.1730\n",
      "Epoch 3/15  Batch 780  G: 4.7560  DX: 0.1472  DY: 0.3444\n",
      "Epoch 3/15  Batch 790  G: 6.1196  DX: 0.3658  DY: 0.2628\n",
      "Epoch 3/15  Batch 800  G: 6.1667  DX: 0.2160  DY: 0.0761\n",
      "Epoch 3/15  Batch 810  G: 9.0815  DX: 0.1143  DY: 0.3603\n",
      "Epoch 3/15  Batch 820  G: 4.1451  DX: 0.4516  DY: 0.3875\n",
      "Epoch 3/15  Batch 830  G: 5.9354  DX: 0.1707  DY: 0.2232\n",
      "Epoch 3/15  Batch 840  G: 4.9158  DX: 0.2549  DY: 0.1457\n",
      "Epoch 3/15  Batch 850  G: 7.0432  DX: 0.1134  DY: 0.4914\n",
      "Epoch 3/15  Batch 860  G: 6.1477  DX: 0.1973  DY: 0.2717\n",
      "Epoch 3/15  Batch 870  G: 12.1418  DX: 0.3681  DY: 0.1929\n",
      "Epoch 3/15  Batch 880  G: 8.2922  DX: 0.3551  DY: 0.3019\n",
      "Epoch 3/15  Batch 890  G: 5.6841  DX: 0.3106  DY: 0.3709\n",
      "Epoch 3/15  Batch 900  G: 6.6153  DX: 0.2743  DY: 0.2530\n",
      "Epoch 3/15  Batch 910  G: 5.0084  DX: 0.2036  DY: 0.1887\n",
      "Epoch 3/15  Batch 920  G: 5.0610  DX: 0.1546  DY: 0.1338\n",
      "Epoch 3/15  Batch 930  G: 6.6869  DX: 0.0470  DY: 0.0587\n",
      "Epoch 3/15  Batch 940  G: 5.7832  DX: 0.1310  DY: 0.2020\n",
      "Epoch 3/15  Batch 950  G: 6.7683  DX: 0.3071  DY: 0.2548\n",
      "Epoch 3/15  Batch 960  G: 3.8243  DX: 0.0908  DY: 0.1032\n",
      "Epoch 3/15  Batch 970  G: 4.5777  DX: 0.1320  DY: 0.2563\n",
      "Epoch 3/15  Batch 980  G: 6.2083  DX: 0.1325  DY: 0.2879\n",
      "Epoch 3/15  Batch 990  G: 6.4822  DX: 0.1659  DY: 0.2040\n",
      "==> Epoch [3/15]  G_loss: 6.2597  D_X_loss: 0.2473  D_Y_loss: 0.2450\n",
      "Epoch 4/15  Batch 10  G: 10.3704  DX: 0.2028  DY: 0.3341\n",
      "Epoch 4/15  Batch 20  G: 5.9190  DX: 0.2026  DY: 0.2836\n",
      "Epoch 4/15  Batch 30  G: 4.6176  DX: 0.2568  DY: 0.2838\n",
      "Epoch 4/15  Batch 40  G: 4.6503  DX: 0.2720  DY: 0.2448\n",
      "Epoch 4/15  Batch 50  G: 4.5192  DX: 0.2805  DY: 0.1987\n",
      "Epoch 4/15  Batch 60  G: 7.5215  DX: 0.2521  DY: 0.4966\n",
      "Epoch 4/15  Batch 70  G: 4.3889  DX: 0.3076  DY: 0.3329\n",
      "Epoch 4/15  Batch 80  G: 7.0298  DX: 0.0852  DY: 0.3226\n",
      "Epoch 4/15  Batch 90  G: 6.6720  DX: 0.1782  DY: 0.3171\n",
      "Epoch 4/15  Batch 100  G: 5.5776  DX: 0.1968  DY: 0.2271\n",
      "Epoch 4/15  Batch 110  G: 3.1969  DX: 0.2223  DY: 0.3722\n",
      "Epoch 4/15  Batch 120  G: 5.2070  DX: 0.2833  DY: 0.1243\n",
      "Epoch 4/15  Batch 130  G: 6.4348  DX: 0.1939  DY: 0.2229\n",
      "Epoch 4/15  Batch 140  G: 4.9719  DX: 0.2439  DY: 0.3560\n",
      "Epoch 4/15  Batch 150  G: 7.3171  DX: 0.3330  DY: 0.3327\n",
      "Epoch 4/15  Batch 160  G: 6.1732  DX: 0.2802  DY: 0.2145\n",
      "Epoch 4/15  Batch 170  G: 5.6481  DX: 0.1369  DY: 0.1300\n",
      "Epoch 4/15  Batch 180  G: 7.4765  DX: 0.1631  DY: 0.2276\n",
      "Epoch 4/15  Batch 190  G: 7.2213  DX: 0.2225  DY: 0.1817\n",
      "Epoch 4/15  Batch 200  G: 6.8226  DX: 0.2073  DY: 0.1034\n",
      "Epoch 4/15  Batch 210  G: 6.8083  DX: 0.3420  DY: 0.2783\n",
      "Epoch 4/15  Batch 220  G: 6.3839  DX: 0.2087  DY: 0.1929\n",
      "Epoch 4/15  Batch 230  G: 6.0151  DX: 0.3336  DY: 0.4408\n",
      "Epoch 4/15  Batch 240  G: 5.2139  DX: 0.2232  DY: 0.3584\n",
      "Epoch 4/15  Batch 250  G: 6.9742  DX: 0.2717  DY: 0.1577\n",
      "Epoch 4/15  Batch 260  G: 4.8426  DX: 0.1233  DY: 0.1916\n",
      "Epoch 4/15  Batch 270  G: 3.9351  DX: 0.1320  DY: 0.1770\n",
      "Epoch 4/15  Batch 280  G: 4.6647  DX: 0.1877  DY: 0.3063\n",
      "Epoch 4/15  Batch 290  G: 4.7094  DX: 0.1995  DY: 0.2257\n",
      "Epoch 4/15  Batch 300  G: 4.3143  DX: 0.1975  DY: 0.1804\n",
      "Epoch 4/15  Batch 310  G: 7.1959  DX: 0.2260  DY: 0.1989\n",
      "Epoch 4/15  Batch 320  G: 5.0545  DX: 0.2226  DY: 0.1274\n",
      "Epoch 4/15  Batch 330  G: 6.8347  DX: 0.1698  DY: 0.2162\n",
      "Epoch 4/15  Batch 340  G: 7.8045  DX: 0.2868  DY: 0.1394\n",
      "Epoch 4/15  Batch 350  G: 8.6424  DX: 0.3069  DY: 0.1736\n",
      "Epoch 4/15  Batch 360  G: 7.9586  DX: 0.2251  DY: 0.2349\n",
      "Epoch 4/15  Batch 370  G: 8.0986  DX: 0.5576  DY: 0.1364\n",
      "Epoch 4/15  Batch 380  G: 6.6155  DX: 0.2955  DY: 0.0959\n",
      "Epoch 4/15  Batch 390  G: 4.4830  DX: 0.2991  DY: 0.2342\n",
      "Epoch 4/15  Batch 400  G: 5.5840  DX: 0.2392  DY: 0.2995\n",
      "Epoch 4/15  Batch 410  G: 5.3460  DX: 0.1991  DY: 0.0640\n",
      "Epoch 4/15  Batch 420  G: 5.8211  DX: 0.3220  DY: 0.3051\n",
      "Epoch 4/15  Batch 430  G: 5.9888  DX: 0.1520  DY: 0.2611\n",
      "Epoch 4/15  Batch 440  G: 7.0637  DX: 0.1292  DY: 0.2288\n",
      "Epoch 4/15  Batch 450  G: 6.6054  DX: 0.0890  DY: 0.1454\n",
      "Epoch 4/15  Batch 460  G: 5.1565  DX: 0.1802  DY: 0.2666\n",
      "Epoch 4/15  Batch 470  G: 5.8236  DX: 0.1329  DY: 0.2386\n",
      "Epoch 4/15  Batch 480  G: 5.8836  DX: 0.2726  DY: 0.1966\n",
      "Epoch 4/15  Batch 490  G: 7.1714  DX: 0.1224  DY: 0.3296\n",
      "Epoch 4/15  Batch 500  G: 4.4109  DX: 0.3679  DY: 0.2876\n",
      "Epoch 4/15  Batch 510  G: 11.7244  DX: 0.2468  DY: 0.3123\n",
      "Epoch 4/15  Batch 520  G: 6.9855  DX: 0.2304  DY: 0.0705\n",
      "Epoch 4/15  Batch 530  G: 5.9526  DX: 0.1561  DY: 0.0815\n",
      "Epoch 4/15  Batch 540  G: 6.4554  DX: 0.1492  DY: 0.2131\n",
      "Epoch 4/15  Batch 550  G: 4.5792  DX: 0.4096  DY: 0.2942\n",
      "Epoch 4/15  Batch 560  G: 5.1648  DX: 0.1405  DY: 0.2020\n",
      "Epoch 4/15  Batch 570  G: 5.6031  DX: 0.2935  DY: 0.2958\n",
      "Epoch 4/15  Batch 580  G: 6.9937  DX: 0.1292  DY: 0.0588\n",
      "Epoch 4/15  Batch 590  G: 7.1313  DX: 0.2527  DY: 0.1453\n",
      "Epoch 4/15  Batch 600  G: 5.6559  DX: 0.1734  DY: 0.2646\n",
      "Epoch 4/15  Batch 610  G: 8.8341  DX: 0.2022  DY: 0.2221\n",
      "Epoch 4/15  Batch 620  G: 4.5628  DX: 0.1426  DY: 0.2705\n",
      "Epoch 4/15  Batch 630  G: 4.8011  DX: 0.1383  DY: 0.1631\n",
      "Epoch 4/15  Batch 640  G: 4.9064  DX: 0.2574  DY: 0.3528\n",
      "Epoch 4/15  Batch 650  G: 6.2716  DX: 0.2700  DY: 0.7442\n",
      "Epoch 4/15  Batch 660  G: 6.0546  DX: 0.2799  DY: 0.1624\n",
      "Epoch 4/15  Batch 670  G: 4.7722  DX: 0.3623  DY: 0.3017\n",
      "Epoch 4/15  Batch 680  G: 7.0741  DX: 0.2611  DY: 0.3782\n",
      "Epoch 4/15  Batch 690  G: 4.9059  DX: 0.3105  DY: 0.1308\n",
      "Epoch 4/15  Batch 700  G: 5.3111  DX: 0.2024  DY: 0.2267\n",
      "Epoch 4/15  Batch 710  G: 3.3022  DX: 0.2490  DY: 0.1593\n",
      "Epoch 4/15  Batch 720  G: 5.5644  DX: 0.2267  DY: 0.2117\n",
      "Epoch 4/15  Batch 730  G: 5.6639  DX: 0.1084  DY: 0.3017\n",
      "Epoch 4/15  Batch 740  G: 5.6374  DX: 0.0725  DY: 0.2822\n",
      "Epoch 4/15  Batch 750  G: 3.4620  DX: 0.2160  DY: 0.2322\n",
      "Epoch 4/15  Batch 760  G: 11.4946  DX: 0.1701  DY: 0.1147\n",
      "Epoch 4/15  Batch 770  G: 4.7520  DX: 0.2749  DY: 0.3193\n",
      "Epoch 4/15  Batch 780  G: 4.4348  DX: 0.2006  DY: 0.3250\n",
      "Epoch 4/15  Batch 790  G: 4.4397  DX: 0.2467  DY: 0.1382\n",
      "Epoch 4/15  Batch 800  G: 4.8557  DX: 0.1317  DY: 0.0951\n",
      "Epoch 4/15  Batch 810  G: 6.0335  DX: 0.2490  DY: 0.2531\n",
      "Epoch 4/15  Batch 820  G: 5.4250  DX: 0.3592  DY: 0.3772\n",
      "Epoch 4/15  Batch 830  G: 4.9347  DX: 0.2153  DY: 0.2100\n",
      "Epoch 4/15  Batch 840  G: 6.8183  DX: 0.1061  DY: 0.1287\n",
      "Epoch 4/15  Batch 850  G: 5.9890  DX: 0.2955  DY: 0.1783\n",
      "Epoch 4/15  Batch 860  G: 7.6679  DX: 0.1208  DY: 0.1617\n",
      "Epoch 4/15  Batch 870  G: 8.2964  DX: 0.1029  DY: 0.2364\n",
      "Epoch 4/15  Batch 880  G: 5.6551  DX: 0.1627  DY: 0.4337\n",
      "Epoch 4/15  Batch 890  G: 7.9412  DX: 0.1344  DY: 0.2173\n",
      "Epoch 4/15  Batch 900  G: 5.2426  DX: 0.1882  DY: 0.4254\n",
      "Epoch 4/15  Batch 910  G: 3.6955  DX: 0.2276  DY: 0.2881\n",
      "Epoch 4/15  Batch 920  G: 5.1264  DX: 0.3348  DY: 0.3045\n",
      "Epoch 4/15  Batch 930  G: 5.5472  DX: 0.1175  DY: 0.1513\n",
      "Epoch 4/15  Batch 940  G: 6.9257  DX: 0.2496  DY: 0.1689\n",
      "Epoch 4/15  Batch 950  G: 5.6961  DX: 0.0513  DY: 0.0623\n",
      "Epoch 4/15  Batch 960  G: 6.3670  DX: 0.3350  DY: 0.1991\n",
      "Epoch 4/15  Batch 970  G: 5.2088  DX: 0.1733  DY: 0.2681\n",
      "Epoch 4/15  Batch 980  G: 4.4485  DX: 0.3594  DY: 0.4104\n",
      "Epoch 4/15  Batch 990  G: 5.3806  DX: 0.2008  DY: 0.2854\n",
      "==> Epoch [4/15]  G_loss: 5.8982  D_X_loss: 0.2406  D_Y_loss: 0.2432\n",
      "Epoch 5/15  Batch 10  G: 4.9042  DX: 0.3464  DY: 0.3272\n",
      "Epoch 5/15  Batch 20  G: 8.0027  DX: 0.1711  DY: 0.2087\n",
      "Epoch 5/15  Batch 30  G: 4.3817  DX: 0.1543  DY: 0.3080\n",
      "Epoch 5/15  Batch 40  G: 8.4321  DX: 0.1155  DY: 0.5632\n",
      "Epoch 5/15  Batch 50  G: 7.8569  DX: 0.2415  DY: 0.1514\n",
      "Epoch 5/15  Batch 60  G: 5.5143  DX: 0.1918  DY: 0.1278\n",
      "Epoch 5/15  Batch 70  G: 3.9785  DX: 0.3977  DY: 0.3638\n",
      "Epoch 5/15  Batch 80  G: 7.1726  DX: 0.1839  DY: 0.1792\n",
      "Epoch 5/15  Batch 90  G: 7.0046  DX: 0.2261  DY: 0.2354\n",
      "Epoch 5/15  Batch 100  G: 4.4871  DX: 0.1943  DY: 0.2024\n",
      "Epoch 5/15  Batch 110  G: 4.3931  DX: 0.1421  DY: 0.1881\n",
      "Epoch 5/15  Batch 120  G: 8.5301  DX: 0.2589  DY: 0.2013\n",
      "Epoch 5/15  Batch 130  G: 5.6717  DX: 0.2093  DY: 0.3314\n",
      "Epoch 5/15  Batch 140  G: 5.5084  DX: 0.3317  DY: 0.2411\n",
      "Epoch 5/15  Batch 150  G: 5.6949  DX: 0.1408  DY: 0.2038\n",
      "Epoch 5/15  Batch 160  G: 4.0836  DX: 0.1950  DY: 0.2763\n",
      "Epoch 5/15  Batch 170  G: 5.2874  DX: 0.0590  DY: 0.1734\n",
      "Epoch 5/15  Batch 180  G: 4.9363  DX: 0.2990  DY: 0.5788\n",
      "Epoch 5/15  Batch 190  G: 5.6111  DX: 0.1959  DY: 0.2111\n",
      "Epoch 5/15  Batch 200  G: 6.1627  DX: 0.1554  DY: 0.1339\n",
      "Epoch 5/15  Batch 210  G: 4.9420  DX: 0.1339  DY: 0.2607\n",
      "Epoch 5/15  Batch 220  G: 7.8582  DX: 0.1008  DY: 0.2180\n",
      "Epoch 5/15  Batch 230  G: 5.3870  DX: 0.7283  DY: 0.2565\n",
      "Epoch 5/15  Batch 240  G: 6.7290  DX: 0.4575  DY: 0.3988\n",
      "Epoch 5/15  Batch 250  G: 5.5327  DX: 0.1462  DY: 0.1673\n",
      "Epoch 5/15  Batch 260  G: 4.9885  DX: 0.1443  DY: 0.2110\n",
      "Epoch 5/15  Batch 270  G: 6.3634  DX: 0.2536  DY: 0.2494\n",
      "Epoch 5/15  Batch 280  G: 6.6020  DX: 0.1476  DY: 0.1400\n",
      "Epoch 5/15  Batch 290  G: 6.8604  DX: 0.4288  DY: 0.2428\n",
      "Epoch 5/15  Batch 300  G: 7.8386  DX: 0.1588  DY: 0.2563\n",
      "Epoch 5/15  Batch 310  G: 7.7733  DX: 0.3186  DY: 0.2467\n",
      "Epoch 5/15  Batch 320  G: 7.9126  DX: 0.1303  DY: 0.0469\n",
      "Epoch 5/15  Batch 330  G: 5.4145  DX: 0.1839  DY: 0.3407\n",
      "Epoch 5/15  Batch 340  G: 8.3043  DX: 0.2398  DY: 0.1907\n",
      "Epoch 5/15  Batch 350  G: 4.9925  DX: 0.1839  DY: 0.2472\n",
      "Epoch 5/15  Batch 360  G: 4.3257  DX: 0.3097  DY: 0.1267\n",
      "Epoch 5/15  Batch 370  G: 5.4211  DX: 0.4079  DY: 0.2581\n",
      "Epoch 5/15  Batch 380  G: 5.3710  DX: 0.1438  DY: 0.2098\n",
      "Epoch 5/15  Batch 390  G: 5.8282  DX: 0.1023  DY: 0.3084\n",
      "Epoch 5/15  Batch 400  G: 5.9726  DX: 0.3628  DY: 0.1552\n",
      "Epoch 5/15  Batch 410  G: 5.3405  DX: 0.2169  DY: 0.2125\n",
      "Epoch 5/15  Batch 420  G: 7.3677  DX: 0.3735  DY: 0.3629\n",
      "Epoch 5/15  Batch 430  G: 4.7062  DX: 0.1203  DY: 0.2200\n",
      "Epoch 5/15  Batch 440  G: 5.9893  DX: 0.1955  DY: 0.1662\n",
      "Epoch 5/15  Batch 450  G: 7.5018  DX: 0.3925  DY: 0.2901\n",
      "Epoch 5/15  Batch 460  G: 4.4500  DX: 0.1706  DY: 0.2564\n",
      "Epoch 5/15  Batch 470  G: 5.1904  DX: 0.3201  DY: 0.3370\n",
      "Epoch 5/15  Batch 480  G: 6.1803  DX: 0.0853  DY: 0.1491\n",
      "Epoch 5/15  Batch 490  G: 4.9601  DX: 0.1033  DY: 0.1880\n",
      "Epoch 5/15  Batch 500  G: 4.8293  DX: 0.1308  DY: 0.5703\n",
      "Epoch 5/15  Batch 510  G: 3.9262  DX: 0.4992  DY: 0.3572\n",
      "Epoch 5/15  Batch 520  G: 4.8297  DX: 0.0590  DY: 0.2451\n",
      "Epoch 5/15  Batch 530  G: 5.5020  DX: 0.1483  DY: 0.2058\n",
      "Epoch 5/15  Batch 540  G: 4.2636  DX: 0.2510  DY: 0.3640\n",
      "Epoch 5/15  Batch 550  G: 6.0310  DX: 0.2018  DY: 0.1447\n",
      "Epoch 5/15  Batch 560  G: 3.5944  DX: 0.2780  DY: 0.3182\n",
      "Epoch 5/15  Batch 570  G: 6.1725  DX: 0.5182  DY: 0.3463\n",
      "Epoch 5/15  Batch 580  G: 3.8315  DX: 0.2371  DY: 0.5726\n",
      "Epoch 5/15  Batch 590  G: 4.1727  DX: 0.2147  DY: 0.3392\n",
      "Epoch 5/15  Batch 600  G: 5.2098  DX: 0.1761  DY: 0.2188\n",
      "Epoch 5/15  Batch 610  G: 5.2155  DX: 0.1092  DY: 0.2739\n",
      "Epoch 5/15  Batch 620  G: 7.5593  DX: 0.1985  DY: 0.3319\n",
      "Epoch 5/15  Batch 630  G: 6.6445  DX: 0.0825  DY: 0.1752\n",
      "Epoch 5/15  Batch 640  G: 5.2745  DX: 0.2030  DY: 0.2794\n",
      "Epoch 5/15  Batch 650  G: 5.6376  DX: 0.2476  DY: 0.3152\n",
      "Epoch 5/15  Batch 660  G: 5.4881  DX: 0.3175  DY: 0.1158\n",
      "Epoch 5/15  Batch 670  G: 6.4995  DX: 0.3125  DY: 0.2850\n",
      "Epoch 5/15  Batch 680  G: 6.9350  DX: 0.2281  DY: 0.2776\n",
      "Epoch 5/15  Batch 690  G: 5.1609  DX: 0.2103  DY: 0.1757\n",
      "Epoch 5/15  Batch 700  G: 4.3843  DX: 0.3361  DY: 0.1952\n",
      "Epoch 5/15  Batch 710  G: 5.6097  DX: 0.1338  DY: 0.2082\n",
      "Epoch 5/15  Batch 720  G: 8.0761  DX: 0.0897  DY: 0.2053\n",
      "Epoch 5/15  Batch 730  G: 4.9065  DX: 0.2016  DY: 0.1710\n",
      "Epoch 5/15  Batch 740  G: 7.3420  DX: 0.2763  DY: 0.3434\n",
      "Epoch 5/15  Batch 750  G: 4.0121  DX: 0.3390  DY: 0.1977\n",
      "Epoch 5/15  Batch 760  G: 6.3951  DX: 0.0814  DY: 0.2140\n",
      "Epoch 5/15  Batch 770  G: 11.8552  DX: 0.3195  DY: 0.1587\n",
      "Epoch 5/15  Batch 780  G: 7.3199  DX: 0.0709  DY: 0.1814\n",
      "Epoch 5/15  Batch 790  G: 5.1951  DX: 0.1937  DY: 0.1418\n",
      "Epoch 5/15  Batch 800  G: 5.7395  DX: 0.1511  DY: 0.2778\n",
      "Epoch 5/15  Batch 810  G: 3.4124  DX: 0.1541  DY: 0.2287\n",
      "Epoch 5/15  Batch 820  G: 5.4382  DX: 0.1197  DY: 0.2279\n",
      "Epoch 5/15  Batch 830  G: 4.0402  DX: 0.2878  DY: 0.1589\n",
      "Epoch 5/15  Batch 840  G: 9.1872  DX: 0.2158  DY: 0.4795\n",
      "Epoch 5/15  Batch 850  G: 4.6451  DX: 0.4227  DY: 0.3408\n",
      "Epoch 5/15  Batch 860  G: 5.9330  DX: 0.2498  DY: 0.5059\n",
      "Epoch 5/15  Batch 870  G: 4.7142  DX: 0.1958  DY: 0.2381\n",
      "Epoch 5/15  Batch 880  G: 4.8870  DX: 0.2670  DY: 0.2791\n",
      "Epoch 5/15  Batch 890  G: 4.5986  DX: 0.3036  DY: 0.2629\n",
      "Epoch 5/15  Batch 900  G: 5.6648  DX: 0.2306  DY: 0.3071\n",
      "Epoch 5/15  Batch 910  G: 6.1849  DX: 0.1709  DY: 0.4384\n",
      "Epoch 5/15  Batch 920  G: 5.2203  DX: 0.1150  DY: 0.2062\n",
      "Epoch 5/15  Batch 930  G: 9.2436  DX: 0.3317  DY: 0.2793\n",
      "Epoch 5/15  Batch 940  G: 5.2577  DX: 0.1317  DY: 0.2060\n",
      "Epoch 5/15  Batch 950  G: 5.5470  DX: 0.2768  DY: 0.2149\n",
      "Epoch 5/15  Batch 960  G: 6.5190  DX: 0.1347  DY: 0.2285\n",
      "Epoch 5/15  Batch 970  G: 6.2212  DX: 0.1207  DY: 0.3051\n",
      "Epoch 5/15  Batch 980  G: 9.5785  DX: 0.0532  DY: 0.1917\n",
      "Epoch 5/15  Batch 990  G: 6.2875  DX: 0.1741  DY: 0.2463\n",
      "==> Epoch [5/15]  G_loss: 5.7383  D_X_loss: 0.2367  D_Y_loss: 0.2507\n",
      "Epoch 6/15  Batch 10  G: 5.7423  DX: 0.1319  DY: 0.1920\n",
      "Epoch 6/15  Batch 20  G: 4.3113  DX: 0.1915  DY: 0.2853\n",
      "Epoch 6/15  Batch 30  G: 5.5691  DX: 0.3327  DY: 0.1619\n",
      "Epoch 6/15  Batch 40  G: 6.5168  DX: 0.3170  DY: 0.2941\n",
      "Epoch 6/15  Batch 50  G: 6.5873  DX: 0.2055  DY: 0.1086\n",
      "Epoch 6/15  Batch 60  G: 5.9648  DX: 0.3552  DY: 0.3006\n",
      "Epoch 6/15  Batch 70  G: 4.7220  DX: 0.2621  DY: 0.2244\n",
      "Epoch 6/15  Batch 80  G: 3.9433  DX: 0.2354  DY: 0.1978\n",
      "Epoch 6/15  Batch 90  G: 4.7721  DX: 0.2756  DY: 0.1707\n",
      "Epoch 6/15  Batch 100  G: 6.4201  DX: 0.1772  DY: 0.1313\n",
      "Epoch 6/15  Batch 110  G: 4.4389  DX: 0.1204  DY: 0.1737\n",
      "Epoch 6/15  Batch 120  G: 7.0239  DX: 0.1398  DY: 0.0818\n",
      "Epoch 6/15  Batch 130  G: 5.1759  DX: 0.2330  DY: 0.2791\n",
      "Epoch 6/15  Batch 140  G: 5.7503  DX: 0.1976  DY: 0.1660\n",
      "Epoch 6/15  Batch 150  G: 4.9520  DX: 0.4040  DY: 0.5405\n",
      "Epoch 6/15  Batch 160  G: 4.0737  DX: 0.2091  DY: 0.2657\n",
      "Epoch 6/15  Batch 170  G: 5.0438  DX: 0.1586  DY: 0.2163\n",
      "Epoch 6/15  Batch 180  G: 5.4923  DX: 0.2118  DY: 0.1226\n",
      "Epoch 6/15  Batch 190  G: 6.9302  DX: 0.4446  DY: 0.2447\n",
      "Epoch 6/15  Batch 200  G: 6.0115  DX: 0.2760  DY: 0.1982\n",
      "Epoch 6/15  Batch 210  G: 5.3561  DX: 0.3651  DY: 0.2447\n",
      "Epoch 6/15  Batch 220  G: 5.0921  DX: 0.2337  DY: 0.3074\n",
      "Epoch 6/15  Batch 230  G: 6.4741  DX: 0.1172  DY: 0.1932\n",
      "Epoch 6/15  Batch 240  G: 4.3308  DX: 0.2514  DY: 0.3065\n",
      "Epoch 6/15  Batch 250  G: 5.6832  DX: 0.2898  DY: 0.2446\n",
      "Epoch 6/15  Batch 260  G: 6.7218  DX: 0.2071  DY: 0.1829\n",
      "Epoch 6/15  Batch 270  G: 3.5431  DX: 0.0589  DY: 0.2182\n",
      "Epoch 6/15  Batch 280  G: 9.0524  DX: 0.3155  DY: 0.3871\n",
      "Epoch 6/15  Batch 290  G: 5.6492  DX: 0.2885  DY: 0.2587\n",
      "Epoch 6/15  Batch 300  G: 6.1111  DX: 0.2402  DY: 0.3755\n",
      "Epoch 6/15  Batch 310  G: 3.8470  DX: 0.1799  DY: 0.2975\n",
      "Epoch 6/15  Batch 320  G: 4.7232  DX: 0.1672  DY: 0.1918\n",
      "Epoch 6/15  Batch 330  G: 6.1239  DX: 0.1672  DY: 0.4347\n",
      "Epoch 6/15  Batch 340  G: 4.5912  DX: 0.2651  DY: 0.2363\n",
      "Epoch 6/15  Batch 350  G: 5.8958  DX: 0.2254  DY: 0.1871\n",
      "Epoch 6/15  Batch 360  G: 3.3406  DX: 0.1302  DY: 0.2501\n",
      "Epoch 6/15  Batch 370  G: 5.0940  DX: 0.0939  DY: 0.2552\n",
      "Epoch 6/15  Batch 380  G: 8.3052  DX: 0.2986  DY: 0.2388\n",
      "Epoch 6/15  Batch 390  G: 4.4607  DX: 0.2477  DY: 0.2799\n",
      "Epoch 6/15  Batch 400  G: 6.1908  DX: 0.0799  DY: 0.1943\n",
      "Epoch 6/15  Batch 410  G: 5.1767  DX: 0.1757  DY: 0.2614\n",
      "Epoch 6/15  Batch 420  G: 4.6538  DX: 0.2356  DY: 0.1516\n",
      "Epoch 6/15  Batch 430  G: 5.6179  DX: 0.2157  DY: 0.2336\n",
      "Epoch 6/15  Batch 440  G: 5.0477  DX: 0.0868  DY: 0.3647\n",
      "Epoch 6/15  Batch 450  G: 6.5045  DX: 0.2706  DY: 0.2046\n",
      "Epoch 6/15  Batch 460  G: 4.5026  DX: 0.1350  DY: 0.2608\n",
      "Epoch 6/15  Batch 470  G: 5.5370  DX: 0.1097  DY: 0.2148\n",
      "Epoch 6/15  Batch 480  G: 4.4083  DX: 0.1606  DY: 0.1285\n",
      "Epoch 6/15  Batch 490  G: 3.9140  DX: 0.3575  DY: 0.3730\n",
      "Epoch 6/15  Batch 500  G: 5.1296  DX: 0.1242  DY: 0.3403\n",
      "Epoch 6/15  Batch 510  G: 4.5619  DX: 0.3898  DY: 0.2858\n",
      "Epoch 6/15  Batch 520  G: 5.5880  DX: 0.2386  DY: 0.3155\n",
      "Epoch 6/15  Batch 530  G: 5.9350  DX: 0.2079  DY: 0.2099\n",
      "Epoch 6/15  Batch 540  G: 4.4657  DX: 0.3678  DY: 0.1687\n",
      "Epoch 6/15  Batch 550  G: 5.5333  DX: 0.2051  DY: 0.2148\n",
      "Epoch 6/15  Batch 560  G: 5.1675  DX: 0.3366  DY: 0.3172\n",
      "Epoch 6/15  Batch 570  G: 5.7004  DX: 0.2607  DY: 0.3341\n",
      "Epoch 6/15  Batch 580  G: 5.0533  DX: 0.3102  DY: 0.2807\n",
      "Epoch 6/15  Batch 590  G: 4.6787  DX: 0.1824  DY: 0.2320\n",
      "Epoch 6/15  Batch 600  G: 6.1781  DX: 0.1392  DY: 0.0958\n",
      "Epoch 6/15  Batch 610  G: 4.9425  DX: 0.1601  DY: 0.2199\n",
      "Epoch 6/15  Batch 620  G: 5.7298  DX: 0.0463  DY: 0.2559\n",
      "Epoch 6/15  Batch 630  G: 7.5516  DX: 0.1388  DY: 0.1383\n",
      "Epoch 6/15  Batch 640  G: 8.7233  DX: 0.2534  DY: 0.4836\n",
      "Epoch 6/15  Batch 650  G: 7.8233  DX: 0.1829  DY: 0.2296\n",
      "Epoch 6/15  Batch 660  G: 4.1459  DX: 0.4208  DY: 0.2045\n",
      "Epoch 6/15  Batch 670  G: 2.9830  DX: 0.2441  DY: 0.2572\n",
      "Epoch 6/15  Batch 680  G: 4.1066  DX: 0.1523  DY: 0.1726\n",
      "Epoch 6/15  Batch 690  G: 5.9674  DX: 0.0803  DY: 0.1085\n",
      "Epoch 6/15  Batch 700  G: 5.1166  DX: 0.1306  DY: 0.3289\n",
      "Epoch 6/15  Batch 710  G: 5.9370  DX: 0.3044  DY: 0.1819\n",
      "Epoch 6/15  Batch 720  G: 4.8117  DX: 0.1026  DY: 0.0787\n",
      "Epoch 6/15  Batch 730  G: 5.8114  DX: 0.3789  DY: 0.4232\n",
      "Epoch 6/15  Batch 740  G: 5.1652  DX: 0.1591  DY: 0.4040\n",
      "Epoch 6/15  Batch 750  G: 4.4462  DX: 0.1661  DY: 0.2534\n",
      "Epoch 6/15  Batch 760  G: 6.0760  DX: 0.3015  DY: 0.0594\n",
      "Epoch 6/15  Batch 770  G: 5.3974  DX: 0.1897  DY: 0.2166\n",
      "Epoch 6/15  Batch 780  G: 7.0589  DX: 0.1140  DY: 0.2272\n",
      "Epoch 6/15  Batch 790  G: 6.5912  DX: 0.0967  DY: 0.1822\n",
      "Epoch 6/15  Batch 800  G: 4.2572  DX: 0.0973  DY: 0.2400\n",
      "Epoch 6/15  Batch 810  G: 5.1392  DX: 0.4117  DY: 0.5120\n",
      "Epoch 6/15  Batch 820  G: 4.3736  DX: 0.4219  DY: 0.4401\n",
      "Epoch 6/15  Batch 830  G: 5.2364  DX: 0.3063  DY: 0.1984\n",
      "Epoch 6/15  Batch 840  G: 6.0707  DX: 0.0725  DY: 0.3023\n",
      "Epoch 6/15  Batch 850  G: 4.3487  DX: 0.1530  DY: 0.3371\n",
      "Epoch 6/15  Batch 860  G: 6.8630  DX: 0.1067  DY: 0.1801\n",
      "Epoch 6/15  Batch 870  G: 4.7894  DX: 0.4022  DY: 0.3255\n",
      "Epoch 6/15  Batch 880  G: 4.3901  DX: 0.1208  DY: 0.1390\n",
      "Epoch 6/15  Batch 890  G: 4.4686  DX: 0.0955  DY: 0.3517\n",
      "Epoch 6/15  Batch 900  G: 4.0161  DX: 0.3083  DY: 0.4976\n",
      "Epoch 6/15  Batch 910  G: 6.0292  DX: 0.1384  DY: 0.2464\n",
      "Epoch 6/15  Batch 920  G: 7.7505  DX: 0.3640  DY: 0.4320\n",
      "Epoch 6/15  Batch 930  G: 5.6486  DX: 0.1840  DY: 0.1169\n",
      "Epoch 6/15  Batch 940  G: 5.2885  DX: 0.0798  DY: 0.1383\n",
      "Epoch 6/15  Batch 950  G: 2.9231  DX: 0.2463  DY: 0.3708\n",
      "Epoch 6/15  Batch 960  G: 5.7379  DX: 0.1898  DY: 0.1672\n",
      "Epoch 6/15  Batch 970  G: 6.9869  DX: 0.1797  DY: 0.3905\n",
      "Epoch 6/15  Batch 980  G: 5.5312  DX: 0.2153  DY: 0.2458\n",
      "Epoch 6/15  Batch 990  G: 5.8648  DX: 0.1405  DY: 0.1096\n",
      "==> Epoch [6/15]  G_loss: 5.4887  D_X_loss: 0.2316  D_Y_loss: 0.2481\n",
      "Epoch 7/15  Batch 10  G: 6.9037  DX: 0.2257  DY: 0.2185\n",
      "Epoch 7/15  Batch 20  G: 4.4490  DX: 0.2247  DY: 0.1428\n",
      "Epoch 7/15  Batch 30  G: 6.2158  DX: 0.2450  DY: 0.2461\n",
      "Epoch 7/15  Batch 40  G: 5.7568  DX: 0.1401  DY: 0.0926\n",
      "Epoch 7/15  Batch 50  G: 4.8715  DX: 0.1872  DY: 0.1828\n",
      "Epoch 7/15  Batch 60  G: 4.4513  DX: 0.1687  DY: 0.1108\n",
      "Epoch 7/15  Batch 70  G: 5.5721  DX: 0.1205  DY: 0.2368\n",
      "Epoch 7/15  Batch 80  G: 9.0405  DX: 0.2886  DY: 0.1544\n",
      "Epoch 7/15  Batch 90  G: 6.5712  DX: 0.2355  DY: 0.1901\n",
      "Epoch 7/15  Batch 100  G: 4.4829  DX: 0.1409  DY: 0.1912\n",
      "Epoch 7/15  Batch 110  G: 6.6149  DX: 0.1994  DY: 0.1199\n",
      "Epoch 7/15  Batch 120  G: 4.3431  DX: 0.2391  DY: 0.1750\n",
      "Epoch 7/15  Batch 130  G: 4.8703  DX: 0.2465  DY: 0.2437\n",
      "Epoch 7/15  Batch 140  G: 6.0157  DX: 0.1235  DY: 0.1511\n",
      "Epoch 7/15  Batch 150  G: 4.5202  DX: 0.1164  DY: 0.2437\n",
      "Epoch 7/15  Batch 160  G: 4.7726  DX: 0.2508  DY: 0.2042\n",
      "Epoch 7/15  Batch 170  G: 3.8834  DX: 0.4040  DY: 0.2814\n",
      "Epoch 7/15  Batch 180  G: 4.2635  DX: 0.1710  DY: 0.2887\n",
      "Epoch 7/15  Batch 190  G: 5.6327  DX: 0.4004  DY: 0.4037\n",
      "Epoch 7/15  Batch 200  G: 5.6572  DX: 0.2435  DY: 0.2483\n",
      "Epoch 7/15  Batch 210  G: 6.8690  DX: 0.2717  DY: 0.2682\n",
      "Epoch 7/15  Batch 220  G: 4.6790  DX: 0.2693  DY: 0.2174\n",
      "Epoch 7/15  Batch 230  G: 5.3356  DX: 0.1748  DY: 0.6714\n",
      "Epoch 7/15  Batch 240  G: 6.0623  DX: 0.2231  DY: 0.2034\n",
      "Epoch 7/15  Batch 250  G: 4.9318  DX: 0.2210  DY: 0.1221\n",
      "Epoch 7/15  Batch 260  G: 4.8017  DX: 0.0573  DY: 0.1442\n",
      "Epoch 7/15  Batch 270  G: 4.4289  DX: 0.2195  DY: 0.1748\n",
      "Epoch 7/15  Batch 280  G: 5.4450  DX: 0.2600  DY: 0.2061\n",
      "Epoch 7/15  Batch 290  G: 5.3176  DX: 0.3018  DY: 0.4509\n",
      "Epoch 7/15  Batch 300  G: 5.5101  DX: 0.1987  DY: 0.2206\n",
      "Epoch 7/15  Batch 310  G: 5.1988  DX: 0.1162  DY: 0.2248\n",
      "Epoch 7/15  Batch 320  G: 5.1735  DX: 0.1641  DY: 0.2988\n",
      "Epoch 7/15  Batch 330  G: 6.9266  DX: 0.1700  DY: 0.0895\n",
      "Epoch 7/15  Batch 340  G: 5.1587  DX: 0.2239  DY: 0.3017\n",
      "Epoch 7/15  Batch 350  G: 6.0813  DX: 0.1933  DY: 0.4792\n",
      "Epoch 7/15  Batch 360  G: 3.6797  DX: 0.1100  DY: 0.1212\n",
      "Epoch 7/15  Batch 370  G: 6.5789  DX: 0.3296  DY: 0.1099\n",
      "Epoch 7/15  Batch 380  G: 4.8162  DX: 0.2663  DY: 0.2067\n",
      "Epoch 7/15  Batch 390  G: 5.3074  DX: 0.3276  DY: 0.2868\n",
      "Epoch 7/15  Batch 400  G: 5.0968  DX: 0.1225  DY: 0.2706\n",
      "Epoch 7/15  Batch 410  G: 8.0362  DX: 0.0924  DY: 0.1392\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m id_Y = G_X2Y(real_Y)\n\u001b[32m     39\u001b[39m loss_id_Y = criterion_identity(id_Y, real_Y)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m id_X = \u001b[43mG_Y2X\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m loss_id_X = criterion_identity(id_X, real_X)\n\u001b[32m     44\u001b[39m loss_identity = (loss_id_X + loss_id_Y) * LAMBDA_ID\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mResnetGenerator.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mResnetBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages/torch/nn/modules/instancenorm.py:124\u001b[39m, in \u001b[36m_InstanceNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m.dim() == \u001b[38;5;28mself\u001b[39m._get_no_batch_dim():\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_no_batch_input(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_instance_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages/torch/nn/modules/instancenorm.py:47\u001b[39m, in \u001b[36m_InstanceNorm._apply_instance_norm\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_apply_instance_norm\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43minstance_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages/torch/nn/functional.py:2867\u001b[39m, in \u001b[36minstance_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps)\u001b[39m\n\u001b[32m   2865\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_input_stats:\n\u001b[32m   2866\u001b[39m     _verify_spatial_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2867\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43minstance_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2868\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2873\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_input_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2874\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2875\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2877\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# DEBUG training loop:\n",
    "# - omejimo število batch-ev na epoh\n",
    "# - sproti izpisujemo napredek\n",
    "# Ko preveriš, da dela, lahko omejitev odstraniš.\n",
    "\n",
    "# %%\n",
    "G_losses = []\n",
    "D_X_losses = []\n",
    "D_Y_losses = []\n",
    "\n",
    "samples_dir = OUT_DIR / \"samples\"\n",
    "samples_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# vzorec za vizualizacijo\n",
    "sample_X = next(iter(trainA_loader)).to(DEVICE)\n",
    "sample_Y = next(iter(trainB_loader)).to(DEVICE)\n",
    "\n",
    "MAX_BATCHES_PER_EPOCH = None  # za debug; kasneje daj na None ali odstrani\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    G_running_loss = 0.0\n",
    "    D_X_running_loss = 0.0\n",
    "    D_Y_running_loss = 0.0\n",
    "\n",
    "    # zipamo loaderja (traja do min dolžine)\n",
    "    for batch_idx, (real_X, real_Y) in enumerate(zip(trainA_loader, trainB_loader), start=1):\n",
    "        real_X = real_X.to(DEVICE)\n",
    "        real_Y = real_Y.to(DEVICE)\n",
    "\n",
    "        # -----------------------\n",
    "        # 1) Generatorji\n",
    "        # -----------------------\n",
    "        set_requires_grad([D_X, D_Y], False)\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # identity\n",
    "        id_Y = G_X2Y(real_Y)\n",
    "        loss_id_Y = criterion_identity(id_Y, real_Y)\n",
    "\n",
    "        id_X = G_Y2X(real_X)\n",
    "        loss_id_X = criterion_identity(id_X, real_X)\n",
    "\n",
    "        loss_identity = (loss_id_X + loss_id_Y) * LAMBDA_ID\n",
    "\n",
    "        # GAN\n",
    "        fake_Y = G_X2Y(real_X)\n",
    "        pred_fake_Y = D_Y(fake_Y)\n",
    "        valid_Y = torch.ones_like(pred_fake_Y, device=DEVICE)\n",
    "        loss_G_X2Y = criterion_GAN(pred_fake_Y, valid_Y)\n",
    "\n",
    "        fake_X = G_Y2X(real_Y)\n",
    "        pred_fake_X = D_X(fake_X)\n",
    "        valid_X = torch.ones_like(pred_fake_X, device=DEVICE)\n",
    "        loss_G_Y2X = criterion_GAN(pred_fake_X, valid_X)\n",
    "\n",
    "        loss_GAN = loss_G_X2Y + loss_G_Y2X\n",
    "\n",
    "        # cycle\n",
    "        rec_X = G_Y2X(fake_Y)\n",
    "        loss_cycle_X = criterion_cycle(rec_X, real_X)\n",
    "\n",
    "        rec_Y = G_X2Y(fake_X)\n",
    "        loss_cycle_Y = criterion_cycle(rec_Y, real_Y)\n",
    "\n",
    "        loss_cycle = (loss_cycle_X + loss_cycle_Y) * LAMBDA_CYCLE\n",
    "\n",
    "        # total G loss\n",
    "        loss_G = loss_GAN + loss_cycle + loss_identity\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # -----------------------\n",
    "        # 2) D_X\n",
    "        # -----------------------\n",
    "        set_requires_grad(D_X, True)\n",
    "        optimizer_D_X.zero_grad()\n",
    "\n",
    "        pred_real_X = D_X(real_X)\n",
    "        valid = torch.ones_like(pred_real_X, device=DEVICE)\n",
    "        loss_D_X_real = criterion_GAN(pred_real_X, valid)\n",
    "\n",
    "        pred_fake_X = D_X(fake_X.detach())\n",
    "        fake = torch.zeros_like(pred_fake_X, device=DEVICE)\n",
    "        loss_D_X_fake = criterion_GAN(pred_fake_X, fake)\n",
    "\n",
    "        loss_D_X_total = 0.5 * (loss_D_X_real + loss_D_X_fake)\n",
    "        loss_D_X_total.backward()\n",
    "        optimizer_D_X.step()\n",
    "\n",
    "        # -----------------------\n",
    "        # 3) D_Y\n",
    "        # -----------------------\n",
    "        set_requires_grad(D_Y, True)\n",
    "        optimizer_D_Y.zero_grad()\n",
    "\n",
    "        pred_real_Y = D_Y(real_Y)\n",
    "        valid = torch.ones_like(pred_real_Y, device=DEVICE)\n",
    "        loss_D_Y_real = criterion_GAN(pred_real_Y, valid)\n",
    "\n",
    "        pred_fake_Y = D_Y(fake_Y.detach())\n",
    "        fake = torch.zeros_like(pred_fake_Y, device=DEVICE)\n",
    "        loss_D_Y_fake = criterion_GAN(pred_fake_Y, fake)\n",
    "\n",
    "        loss_D_Y_total = 0.5 * (loss_D_Y_real + loss_D_Y_fake)\n",
    "        loss_D_Y_total.backward()\n",
    "        optimizer_D_Y.step()\n",
    "\n",
    "        # akumulacija izgub\n",
    "        G_running_loss += loss_G.item()\n",
    "        D_X_running_loss += loss_D_X_total.item()\n",
    "        D_Y_running_loss += loss_D_Y_total.item()\n",
    "\n",
    "        # DEBUG print\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch}/{NUM_EPOCHS}  Batch {batch_idx}  \"\n",
    "                  f\"G: {loss_G.item():.4f}  DX: {loss_D_X_total.item():.4f}  DY: {loss_D_Y_total.item():.4f}\")\n",
    "\n",
    "        # Omejimo št. batch-ev na epoh za debug\n",
    "        if MAX_BATCHES_PER_EPOCH is not None and batch_idx >= MAX_BATCHES_PER_EPOCH:\n",
    "            break\n",
    "\n",
    "    # Povprečja po epohi\n",
    "    num_batches = min(len(trainA_loader), len(trainB_loader))\n",
    "    if MAX_BATCHES_PER_EPOCH is not None:\n",
    "        num_batches = min(num_batches, MAX_BATCHES_PER_EPOCH)\n",
    "\n",
    "    G_epoch_loss = G_running_loss / num_batches\n",
    "    D_X_epoch_loss = D_X_running_loss / num_batches\n",
    "    D_Y_epoch_loss = D_Y_running_loss / num_batches\n",
    "\n",
    "    G_losses.append(G_epoch_loss)\n",
    "    D_X_losses.append(D_X_epoch_loss)\n",
    "    D_Y_losses.append(D_Y_epoch_loss)\n",
    "\n",
    "    print(f\"==> Epoch [{epoch}/{NUM_EPOCHS}]  \"\n",
    "          f\"G_loss: {G_epoch_loss:.4f}  \"\n",
    "          f\"D_X_loss: {D_X_epoch_loss:.4f}  \"\n",
    "          f\"D_Y_loss: {D_Y_epoch_loss:.4f}\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Vizualizacija po epohi\n",
    "    # -----------------------\n",
    "    with torch.no_grad():\n",
    "        G_X2Y.eval()\n",
    "        G_Y2X.eval()\n",
    "\n",
    "        # apple -> orange -> apple\n",
    "        fake_Y_sample = G_X2Y(sample_X)\n",
    "        rec_X_sample = G_Y2X(fake_Y_sample)\n",
    "\n",
    "        # orange -> apple -> orange\n",
    "        fake_X_sample = G_Y2X(sample_Y)\n",
    "        rec_Y_sample = G_X2Y(fake_X_sample)\n",
    "\n",
    "        G_X2Y.train()\n",
    "        G_Y2X.train()\n",
    "\n",
    "        sX = denormalize(sample_X.cpu())\n",
    "        fY = denormalize(fake_Y_sample.cpu())\n",
    "        rX = denormalize(rec_X_sample.cpu())\n",
    "\n",
    "        sY = denormalize(sample_Y.cpu())\n",
    "        fX = denormalize(fake_X_sample.cpu())\n",
    "        rY = denormalize(rec_Y_sample.cpu())\n",
    "\n",
    "        def concat3(a, b, c):\n",
    "            return torch.cat([a, b, c], dim=3)\n",
    "\n",
    "        grid_X = concat3(sX, fY, rX)\n",
    "        grid_Y = concat3(sY, fX, rY)\n",
    "        vis = torch.cat([grid_X, grid_Y], dim=2)\n",
    "\n",
    "        vis_img = vutils.make_grid(vis, nrow=1)\n",
    "        vis_img = vis_img.permute(1, 2, 0).numpy()\n",
    "\n",
    "        epoch_img_path = samples_dir / f\"epoch_{epoch:03d}.png\"\n",
    "        plt.imsave(epoch_img_path, vis_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Ustvarimo generatorja in diskriminatorja ter inicializiramo uteži.\n",
    "\n",
    "# %%\n",
    "G_X2Y = ResnetGenerator(input_nc=3, output_nc=3, n_filters=64, n_blocks=6).to(DEVICE)\n",
    "G_Y2X = ResnetGenerator(input_nc=3, output_nc=3, n_filters=64, n_blocks=6).to(DEVICE)\n",
    "\n",
    "D_X = PatchDiscriminator(input_nc=3, n_filters=64).to(DEVICE)  # real vs fake apples\n",
    "D_Y = PatchDiscriminator(input_nc=3, n_filters=64).to(DEVICE)  # real vs fake oranges\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif isinstance(m, nn.InstanceNorm2d):\n",
    "        if m.weight is not None:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "G_X2Y.apply(init_weights)\n",
    "G_Y2X.apply(init_weights)\n",
    "D_X.apply(init_weights)\n",
    "D_Y.apply(init_weights)\n",
    "\n",
    "print(\"Models initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Loss funkcije:\n",
    "# - GAN: MSE (LSGAN)\n",
    "# - Cycle: L1\n",
    "# - Identity: L1\n",
    "# \n",
    "# Optimizatorji: Adam, betas = (0.5, 0.999)\n",
    "\n",
    "# %%\n",
    "# GAN loss (LSGAN)\n",
    "criterion_GAN = nn.MSELoss()\n",
    "# Cycle & identity loss\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "# Optimizatorji\n",
    "optimizer_G = optim.Adam(\n",
    "    itertools.chain(G_X2Y.parameters(), G_Y2X.parameters()),\n",
    "    lr=LR, betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_D_X = optim.Adam(D_X.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "optimizer_D_Y = optim.Adam(D_Y.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "def set_requires_grad(nets, requires_grad=False):\n",
    "    \"\"\"\n",
    "    Omogoči/onemogoči gradiente za podane mreže.\n",
    "    \"\"\"\n",
    "    if not isinstance(nets, list):\n",
    "        nets = [nets]\n",
    "    for net in nets:\n",
    "        for p in net.parameters():\n",
    "            p.requires_grad = requires_grad\n",
    "\n",
    "\n",
    "def denormalize(tensor):\n",
    "    \"\"\"\n",
    "    Pretvori iz [-1, 1] v [0, 1] za prikaz/shranjevanje.\n",
    "    \"\"\"\n",
    "    out = tensor * 0.5 + 0.5\n",
    "    return torch.clamp(out, 0.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Glavni training loop:\n",
    "# - posodabljamo G_X2Y, G_Y2X, D_X, D_Y\n",
    "# - računamo GAN, cycle in identity loss\n",
    "# - po vsaki epohi shranimo vizualizacijo (X->Y->X, Y->X->Y)\n",
    "\n",
    "# %%\n",
    "G_losses = []\n",
    "D_X_losses = []\n",
    "D_Y_losses = []\n",
    "\n",
    "samples_dir = OUT_DIR / \"samples\"\n",
    "samples_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Vzorec za vizualizacijo (vzemi iz treninga, da sigurno obstaja)\n",
    "sample_X = next(iter(trainA_loader)).to(DEVICE)  # apple\n",
    "sample_Y = next(iter(trainB_loader)).to(DEVICE)  # orange\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    G_running_loss = 0.0\n",
    "    D_X_running_loss = 0.0\n",
    "    D_Y_running_loss = 0.0\n",
    "\n",
    "    # zipamo loaderja (traja do min dolžine)\n",
    "    for real_X, real_Y in zip(trainA_loader, trainB_loader):\n",
    "        real_X = real_X.to(DEVICE)  # jabolka (X)\n",
    "        real_Y = real_Y.to(DEVICE)  # pomaranče (Y)\n",
    "\n",
    "        # -----------------------\n",
    "        # 1) Treniramo generatorja\n",
    "        # -----------------------\n",
    "        set_requires_grad([D_X, D_Y], False)\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identity loss: G_X2Y(Y) ≈ Y, G_Y2X(X) ≈ X\n",
    "        id_Y = G_X2Y(real_Y)\n",
    "        loss_id_Y = criterion_identity(id_Y, real_Y)\n",
    "\n",
    "        id_X = G_Y2X(real_X)\n",
    "        loss_id_X = criterion_identity(id_X, real_X)\n",
    "\n",
    "        loss_identity = (loss_id_X + loss_id_Y) * LAMBDA_ID\n",
    "\n",
    "        # GAN loss:\n",
    "        # X -> Y\n",
    "        fake_Y = G_X2Y(real_X)\n",
    "        pred_fake_Y = D_Y(fake_Y)\n",
    "        valid_Y = torch.ones_like(pred_fake_Y, device=DEVICE)\n",
    "        loss_G_X2Y = criterion_GAN(pred_fake_Y, valid_Y)\n",
    "\n",
    "        # Y -> X\n",
    "        fake_X = G_Y2X(real_Y)\n",
    "        pred_fake_X = D_X(fake_X)\n",
    "        valid_X = torch.ones_like(pred_fake_X, device=DEVICE)\n",
    "        loss_G_Y2X = criterion_GAN(pred_fake_X, valid_X)\n",
    "\n",
    "        loss_GAN = loss_G_X2Y + loss_G_Y2X\n",
    "\n",
    "        # Cycle loss:\n",
    "        # X -> Y -> X\n",
    "        rec_X = G_Y2X(fake_Y)\n",
    "        loss_cycle_X = criterion_cycle(rec_X, real_X)\n",
    "\n",
    "        # Y -> X -> Y\n",
    "        rec_Y = G_X2Y(fake_X)\n",
    "        loss_cycle_Y = criterion_cycle(rec_Y, real_Y)\n",
    "\n",
    "        loss_cycle = (loss_cycle_X + loss_cycle_Y) * LAMBDA_CYCLE\n",
    "\n",
    "        # Skupni loss za generatorja\n",
    "        loss_G = loss_GAN + loss_cycle + loss_identity\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # -----------------------\n",
    "        # 2) Treniramo D_X (apples)\n",
    "        # -----------------------\n",
    "        set_requires_grad(D_X, True)\n",
    "        optimizer_D_X.zero_grad()\n",
    "\n",
    "        # Real X\n",
    "        pred_real_X = D_X(real_X)\n",
    "        valid = torch.ones_like(pred_real_X, device=DEVICE)\n",
    "        loss_D_X_real = criterion_GAN(pred_real_X, valid)\n",
    "\n",
    "        # Fake X\n",
    "        pred_fake_X = D_X(fake_X.detach())\n",
    "        fake = torch.zeros_like(pred_fake_X, device=DEVICE)\n",
    "        loss_D_X_fake = criterion_GAN(pred_fake_X, fake)\n",
    "\n",
    "        loss_D_X_total = 0.5 * (loss_D_X_real + loss_D_X_fake)\n",
    "        loss_D_X_total.backward()\n",
    "        optimizer_D_X.step()\n",
    "\n",
    "        # -----------------------\n",
    "        # 3) Treniramo D_Y (oranges)\n",
    "        # -----------------------\n",
    "        set_requires_grad(D_Y, True)\n",
    "        optimizer_D_Y.zero_grad()\n",
    "\n",
    "        # Real Y\n",
    "        pred_real_Y = D_Y(real_Y)\n",
    "        valid = torch.ones_like(pred_real_Y, device=DEVICE)\n",
    "        loss_D_Y_real = criterion_GAN(pred_real_Y, valid)\n",
    "\n",
    "        # Fake Y\n",
    "        pred_fake_Y = D_Y(fake_Y.detach())\n",
    "        fake = torch.zeros_like(pred_fake_Y, device=DEVICE)\n",
    "        loss_D_Y_fake = criterion_GAN(pred_fake_Y, fake)\n",
    "\n",
    "        loss_D_Y_total = 0.5 * (loss_D_Y_real + loss_D_Y_fake)\n",
    "        loss_D_Y_total.backward()\n",
    "        optimizer_D_Y.step()\n",
    "\n",
    "        # Akumulacija izgub\n",
    "        G_running_loss += loss_G.item()\n",
    "        D_X_running_loss += loss_D_X_total.item()\n",
    "        D_Y_running_loss += loss_D_Y_total.item()\n",
    "\n",
    "    # Povprečne izgube po epohi\n",
    "    G_epoch_loss = G_running_loss / len(trainA_loader)\n",
    "    D_X_epoch_loss = D_X_running_loss / len(trainA_loader)\n",
    "    D_Y_epoch_loss = D_Y_running_loss / len(trainA_loader)\n",
    "\n",
    "    G_losses.append(G_epoch_loss)\n",
    "    D_X_losses.append(D_X_epoch_loss)\n",
    "    D_Y_losses.append(D_Y_epoch_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{NUM_EPOCHS}]  \"\n",
    "          f\"G_loss: {G_epoch_loss:.4f}  \"\n",
    "          f\"D_X_loss: {D_X_epoch_loss:.4f}  \"\n",
    "          f\"D_Y_loss: {D_Y_epoch_loss:.4f}\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Vizualizacija po epohi\n",
    "    # -----------------------\n",
    "    with torch.no_grad():\n",
    "        G_X2Y.eval()\n",
    "        G_Y2X.eval()\n",
    "\n",
    "        # apple -> orange -> apple\n",
    "        fake_Y_sample = G_X2Y(sample_X)\n",
    "        rec_X_sample = G_Y2X(fake_Y_sample)\n",
    "\n",
    "        # orange -> apple -> orange\n",
    "        fake_X_sample = G_Y2X(sample_Y)\n",
    "        rec_Y_sample = G_X2Y(fake_X_sample)\n",
    "\n",
    "        G_X2Y.train()\n",
    "        G_Y2X.train()\n",
    "\n",
    "        # denormalizacija\n",
    "        sX = denormalize(sample_X.cpu())\n",
    "        fY = denormalize(fake_Y_sample.cpu())\n",
    "        rX = denormalize(rec_X_sample.cpu())\n",
    "\n",
    "        sY = denormalize(sample_Y.cpu())\n",
    "        fX = denormalize(fake_X_sample.cpu())\n",
    "        rY = denormalize(rec_Y_sample.cpu())\n",
    "\n",
    "        # Funkcija za združevanje 3 slik vodoravno\n",
    "        def concat3(a, b, c):\n",
    "            return torch.cat([a, b, c], dim=3)\n",
    "\n",
    "        grid_X = concat3(sX, fY, rX)\n",
    "        grid_Y = concat3(sY, fX, rY)\n",
    "\n",
    "        # skupaj X in Y vertikalno\n",
    "        vis = torch.cat([grid_X, grid_Y], dim=2)  # [B,C,H_total,W_total]\n",
    "\n",
    "        vis_img = vutils.make_grid(vis, nrow=1)\n",
    "        vis_img = vis_img.permute(1, 2, 0).numpy()\n",
    "\n",
    "        epoch_img_path = samples_dir / f\"epoch_{epoch:03d}.png\"\n",
    "        plt.imsave(epoch_img_path, vis_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Grafa:\n",
    "# - generator loss\n",
    "# - discriminator losses (D_X, D_Y)\n",
    "\n",
    "# %%\n",
    "plt.figure()\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), G_losses, label=\"Generator loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Generator loss (CycleGAN)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(OUT_DIR / \"generator_loss.png\", dpi=150)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), D_X_losses, label=\"D_X loss\")\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), D_Y_losses, label=\"D_Y loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Discriminator losses (D_X, D_Y)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(OUT_DIR / \"discriminator_losses.png\", dpi=150)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc147d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Iz shranjenih epoch_XXX.png naredimo animacijo traininga (GIF).\n",
    "\n",
    "# %%\n",
    "frames = []\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    img_path = samples_dir / f\"epoch_{epoch:03d}.png\"\n",
    "    if img_path.exists():\n",
    "        frames.append(imageio.imread(img_path))\n",
    "\n",
    "gif_path = OUT_DIR / \"training_progress.gif\"\n",
    "if frames:\n",
    "    imageio.mimsave(gif_path, frames, fps=2)\n",
    "    print(\"Saved GIF to:\", gif_path)\n",
    "else:\n",
    "    print(\"No frames found for GIF.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
