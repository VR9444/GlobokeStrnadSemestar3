{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b066493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ale-py in /Users/viktorrackov/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages (0.11.2)\n",
      "Requirement already satisfied: opencv-python in /Users/viktorrackov/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: imageio in /Users/viktorrackov/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages (2.37.0)\n",
      "Requirement already satisfied: gymnasium[atari] in /Users/viktorrackov/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/viktorrackov/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages (from gymnasium[atari]) (2.2.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/viktorrackov/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages (from gymnasium[atari]) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/viktorrackov/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages (from gymnasium[atari]) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/viktorrackov/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages (from gymnasium[atari]) (0.0.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /Users/viktorrackov/Desktop/Feri/3.semestar/Globoke/.venv/lib/python3.13/site-packages (from imageio) (12.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"gymnasium[atari]\" ale-py opencv-python imageio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36a563cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports & basic setup\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "from typing import Deque, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import cv2\n",
    "\n",
    "# registriramo ALE env-e\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer: Deque[Tuple[np.ndarray, int, float, np.ndarray, bool]] = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # state, next_state: (4, 84, 84)\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states      = np.array(states, dtype=np.float32)\n",
    "        next_states = np.array(next_states, dtype=np.float32)\n",
    "        actions     = np.array(actions, dtype=np.int64)\n",
    "        rewards     = np.array(rewards, dtype=np.float32)\n",
    "        dones       = np.array(dones, dtype=np.float32)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae3ec743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkCNN(nn.Module):\n",
    "    def __init__(self, num_actions: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),  # 4x84x84 -> 32x20x20\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # 64x9x9\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), # 64x7x7\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 512),  # 3136 -> 512\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: (batch, 4, 84, 84)\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed8d119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions: int,\n",
    "        gamma: float = 0.99,\n",
    "        lr: float = 1e-4,\n",
    "        batch_size: int = 32,\n",
    "        buffer_capacity: int = 200_000,\n",
    "        epsilon_start: float = 1.0,\n",
    "        epsilon_end: float = 0.01,\n",
    "        epsilon_decay: float = 0.01,  # faktor za eksponentni decay\n",
    "        target_update_every: int = 10_000,  # v korakih\n",
    "        ddqn: bool = False,\n",
    "    ):\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.ddqn = ddqn\n",
    "\n",
    "        self.q_net = QNetworkCNN(num_actions).to(device)\n",
    "        self.target_net = QNetworkCNN(num_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(capacity=buffer_capacity)\n",
    "\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        self.target_update_every = target_update_every\n",
    "        self.train_steps = 0\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        # state: (4, 84, 84)\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.num_actions)\n",
    "\n",
    "        state_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state_t)\n",
    "        return q_values.argmax(dim=1).item()\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        # eksponenten decay: eps = eps_end + (eps_start - eps_end)*exp(-decay * t)\n",
    "        self.epsilon = max(\n",
    "            self.epsilon_end,\n",
    "            self.epsilon_end + (self.epsilon_start - self.epsilon_end) * np.exp(-self.epsilon_decay * self.train_steps),\n",
    "        )\n",
    "\n",
    "    def push(self, *transition):\n",
    "        self.replay_buffer.push(*transition)\n",
    "\n",
    "    def can_learn(self):\n",
    "        return len(self.replay_buffer) >= self.batch_size\n",
    "\n",
    "    def learn(self):\n",
    "        if not self.can_learn():\n",
    "            return None\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        states_t      = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "        next_states_t = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "        actions_t     = torch.tensor(actions, dtype=torch.int64, device=device)\n",
    "        rewards_t     = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "        dones_t       = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Q(s,a)\n",
    "        q_values = self.q_net(states_t).gather(1, actions_t.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if self.ddqn:\n",
    "                # DDQN: akcije iz online mreže, vrednosti iz target mreže\n",
    "                next_q_online = self.q_net(next_states_t)\n",
    "                next_actions = next_q_online.argmax(dim=1)  # (batch,)\n",
    "                next_q_target = self.target_net(next_states_t).gather(\n",
    "                    1, next_actions.unsqueeze(1)\n",
    "                ).squeeze(1)\n",
    "                target_q = rewards_t + self.gamma * next_q_target * (1.0 - dones_t)\n",
    "            else:\n",
    "                # DQN: max_a Q_target(s', a)\n",
    "                next_q_values = self.target_net(next_states_t).max(dim=1)[0]\n",
    "                target_q = rewards_t + self.gamma * next_q_values * (1.0 - dones_t)\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, target_q)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.q_net.parameters(), 10.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.train_steps += 1\n",
    "        self.update_epsilon()\n",
    "\n",
    "        # posodobitev target mreže\n",
    "        if self.train_steps % self.target_update_every == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def save(self, path: str):\n",
    "        torch.save(self.q_net.state_dict(), path)\n",
    "\n",
    "    def load(self, path: str):\n",
    "        self.q_net.load_state_dict(torch.load(path, map_location=device))\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e55b76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and frame stacking for Pong\n",
    "\n",
    "def preprocess_frame(frame: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    frame: (H, W, 3) uint8, RGB\n",
    "    return: (84, 84) float32, [0,1]\n",
    "    \"\"\"\n",
    "    # convert to grayscale\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    # resize to 84x84\n",
    "    frame = cv2.resize(frame, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "    frame = frame.astype(np.float32) / 255.0\n",
    "    return frame\n",
    "\n",
    "\n",
    "class FrameStackEnv:\n",
    "    \"\"\"\n",
    "    Wraps Atari env:\n",
    "    - preprocess to 84x84 grayscale\n",
    "    - stack last 4 frames into (4,84,84) state\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, k: int = 4):\n",
    "        self.env = env\n",
    "        self.k = k\n",
    "        self.frames: Deque[np.ndarray] = deque(maxlen=k)\n",
    "\n",
    "        # action space is the same\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        frame = preprocess_frame(obs)\n",
    "        self.frames.clear()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(frame)\n",
    "        state = np.stack(self.frames, axis=0)  # (4,84,84)\n",
    "        return state, info\n",
    "\n",
    "    def step(self, action: int):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        frame = preprocess_frame(obs)\n",
    "        self.frames.append(frame)\n",
    "        state = np.stack(self.frames, axis=0)\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "def make_pong_env(render_mode=None, repeat_action_probability=0.25, frameskip=4):\n",
    "    \"\"\"\n",
    "    ALE/Pong-v5, po farama dokumentaciji.\n",
    "    \"\"\"\n",
    "    env = gym.make(\n",
    "        \"ALE/Pong-v5\",\n",
    "        obs_type=\"rgb\",\n",
    "        frameskip=frameskip,\n",
    "        repeat_action_probability=repeat_action_probability,\n",
    "        render_mode=render_mode,\n",
    "    )\n",
    "    env = FrameStackEnv(env, k=4)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f69b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pong(\n",
    "    ddqn: bool,\n",
    "    num_episodes: int = 1000,\n",
    "    max_steps_per_episode: int = 18_000,\n",
    "    solved_score: float = 18.0,\n",
    "):\n",
    "    # env brez renderja za trening\n",
    "    env = make_pong_env(render_mode=None)\n",
    "\n",
    "    num_actions = env.action_space.n\n",
    "    print(\"Num actions:\", num_actions)\n",
    "\n",
    "    agent = DQNAgent(\n",
    "        num_actions=num_actions,\n",
    "        gamma=0.99,\n",
    "        lr=1e-4,\n",
    "        batch_size=32,\n",
    "        buffer_capacity=200_000,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.01,\n",
    "        target_update_every=10_000,\n",
    "        ddqn=ddqn,\n",
    "    )\n",
    "\n",
    "    episode_rewards: List[float] = []\n",
    "    moving_avg: List[float] = []\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        for t in range(max_steps_per_episode):\n",
    "            global_step += 1\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, info_step = env.step(action)\n",
    "\n",
    "            agent.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            loss = agent.learn()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        if len(episode_rewards) >= 20:\n",
    "            avg = np.mean(episode_rewards[-20:])\n",
    "        else:\n",
    "            avg = np.mean(episode_rewards)\n",
    "        moving_avg.append(avg)\n",
    "\n",
    "        if episode % 10 == 0 or episode == 1:\n",
    "            print(\n",
    "                f\"Episode {episode}/{num_episodes} | \"\n",
    "                f\"Reward: {episode_reward:.1f} | \"\n",
    "                f\"Avg(20): {avg:.2f} | \"\n",
    "                f\"Epsilon: {agent.epsilon:.3f}\"\n",
    "            )\n",
    "\n",
    "        # opcijski stop pogoji\n",
    "        if avg >= solved_score and episode >= 100:\n",
    "            print(f\"Solved Pong with avg reward {avg:.2f} at episode {episode}\")\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return agent, episode_rewards, moving_avg\n",
    "\n",
    "\n",
    "def plot_rewards(rewards: List[float], moving_avg: List[float], title: str):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards, label=\"Reward per episode\")\n",
    "    plt.plot(moving_avg, label=\"Moving average (last 20)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd74c08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num actions: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000 | Reward: -21.0 | Avg(20): -21.00 | Epsilon: 0.010\n"
     ]
    }
   ],
   "source": [
    "# DQN on Pong\n",
    "dqn_agent, dqn_rewards, dqn_mavg = run_pong(\n",
    "    ddqn=False,\n",
    "    num_episodes=1000,\n",
    "    max_steps_per_episode=18_000,\n",
    "    solved_score=18.0,\n",
    ")\n",
    "plot_rewards(dqn_rewards, dqn_mavg, \"DQN on Pong\")\n",
    "\n",
    "# Shrani model (za GIF)\n",
    "dqn_agent.save(\"V4.4-Viktor-Rackov-DQN-Pong.pt\")\n",
    "np.save(\"V4.4-Viktor-Rackov-DQN-Pong-rewards.npy\", np.array(dqn_rewards))\n",
    "\n",
    "# DDQN on Pong\n",
    "ddqn_agent, ddqn_rewards, ddqn_mavg = run_pong(\n",
    "    ddqn=True,\n",
    "    num_episodes=1000,\n",
    "    max_steps_per_episode=18_000,\n",
    "    solved_score=18.0,\n",
    ")\n",
    "plot_rewards(ddqn_rewards, ddqn_mavg, \"DDQN on Pong\")\n",
    "\n",
    "ddqn_agent.save(\"V4.4-Viktor-Rackov-DDQN-Pong.pt\")\n",
    "np.save(\"V4.4-Viktor-Rackov-DDQN-Pong-rewards.npy\", np.array(ddqn_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gif(\n",
    "    agent: DQNAgent,\n",
    "    filename: str,\n",
    "    max_steps_per_episode: int = 18_000,\n",
    "    fps: int = 30,\n",
    "):\n",
    "    env = make_pong_env(render_mode=\"rgb_array\")\n",
    "\n",
    "    frames = []\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done and len(frames) < max_steps_per_episode:\n",
    "        # deterministična politika (brez epsilona)\n",
    "        state_t = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = agent.q_net(state_t)\n",
    "        action = q_values.argmax(dim=1).item()\n",
    "\n",
    "        # render frame BEFORE step (ali po desire)\n",
    "        raw_env = env.env.env  # FrameStackEnv.env -> ALE env\n",
    "        frame = raw_env.render()\n",
    "        frames.append(frame)\n",
    "\n",
    "        next_state, reward, done, info_step = env.step(action)\n",
    "        state = next_state\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # save gif\n",
    "    imageio.mimsave(filename, frames, fps=fps)\n",
    "    print(f\"Saved GIF to {filename}\")\n",
    "\n",
    "\n",
    "# Primer: naredi GIF za DDQN agenta (če ti je ta boljši)\n",
    "make_gif(\n",
    "    ddqn_agent,\n",
    "    filename=\"V4.4-Viktor-Rackov-Pong-DDQN.gif\",\n",
    "    max_steps_per_episode=18_000,\n",
    "    fps=30,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146f687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_gif(\n",
    "    dqn_agent,\n",
    "    filename=\"V4.4-Viktor-Rackov-Pong-DQN.gif\",\n",
    "    max_steps_per_episode=18_000,\n",
    "    fps=30,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
